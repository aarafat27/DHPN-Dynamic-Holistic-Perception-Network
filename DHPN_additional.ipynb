{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pt8CTQLd004Z",
        "outputId": "88ad4c69-e39c-4431-cb9f-0f44e4304e3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Model initialized.\n",
            "DynamicHolisticPerceptionNetwork(\n",
            "  (initial_conv): Sequential(\n",
            "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (global_extractor): GlobalFeatureExtractor(\n",
            "    (layer): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (se): SqueezeExcitation(\n",
            "          (fc1): Linear(in_features=256, out_features=16, bias=False)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (fc2): Linear(in_features=16, out_features=256, bias=False)\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropblock): DropBlock()\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (se): SqueezeExcitation(\n",
            "          (fc1): Linear(in_features=256, out_features=16, bias=False)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (fc2): Linear(in_features=16, out_features=256, bias=False)\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropblock): DropBlock()\n",
            "      )\n",
            "      (2): ResidualBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (se): SqueezeExcitation(\n",
            "          (fc1): Linear(in_features=256, out_features=16, bias=False)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (fc2): Linear(in_features=16, out_features=256, bias=False)\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropblock): DropBlock()\n",
            "      )\n",
            "      (3): ResidualBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (se): SqueezeExcitation(\n",
            "          (fc1): Linear(in_features=256, out_features=16, bias=False)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (fc2): Linear(in_features=16, out_features=256, bias=False)\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropblock): DropBlock()\n",
            "      )\n",
            "    )\n",
            "    (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            "  (local_extractor): LocalFeatureExtractor(\n",
            "    (layer): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (se): SqueezeExcitation(\n",
            "          (fc1): Linear(in_features=256, out_features=16, bias=False)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (fc2): Linear(in_features=16, out_features=256, bias=False)\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropblock): DropBlock()\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (se): SqueezeExcitation(\n",
            "          (fc1): Linear(in_features=256, out_features=16, bias=False)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (fc2): Linear(in_features=16, out_features=256, bias=False)\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropblock): DropBlock()\n",
            "      )\n",
            "      (2): ResidualBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (se): SqueezeExcitation(\n",
            "          (fc1): Linear(in_features=256, out_features=16, bias=False)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (fc2): Linear(in_features=16, out_features=256, bias=False)\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropblock): DropBlock()\n",
            "      )\n",
            "      (3): ResidualBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (se): SqueezeExcitation(\n",
            "          (fc1): Linear(in_features=256, out_features=16, bias=False)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (fc2): Linear(in_features=16, out_features=256, bias=False)\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropblock): DropBlock()\n",
            "      )\n",
            "    )\n",
            "    (pool): AdaptiveAvgPool2d(output_size=(2, 2))\n",
            "  )\n",
            "  (adaptive_attention): EnhancedAdaptiveAttentionModule(\n",
            "    (query): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (key): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (value): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (multihead_attn): MultiheadAttention(\n",
            "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "    (fc): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Linear(in_features=128, out_features=2, bias=True)\n",
            "      (3): Softmax(dim=1)\n",
            "    )\n",
            "  )\n",
            "  (fusion_fc): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=256, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch [1/200] - Loss: 2.2543 - Train Acc: 14.82% - Val Acc: 22.60% - Val Loss: 1.9948 - Time: 62.24s\n",
            "Best model saved with Val Acc: 22.60%\n",
            "Epoch [2/200] - Loss: 2.1369 - Train Acc: 21.94% - Val Acc: 40.33% - Val Loss: 1.6748 - Time: 60.66s\n",
            "Best model saved with Val Acc: 40.33%\n",
            "Epoch [3/200] - Loss: 2.0632 - Train Acc: 27.79% - Val Acc: 43.42% - Val Loss: 1.5869 - Time: 61.29s\n",
            "Best model saved with Val Acc: 43.42%\n",
            "Epoch [4/200] - Loss: 2.0139 - Train Acc: 31.34% - Val Acc: 52.02% - Val Loss: 1.3862 - Time: 60.58s\n",
            "Best model saved with Val Acc: 52.02%\n",
            "Epoch [5/200] - Loss: 1.9997 - Train Acc: 32.52% - Val Acc: 54.32% - Val Loss: 1.3645 - Time: 59.78s\n",
            "Best model saved with Val Acc: 54.32%\n",
            "Epoch [6/200] - Loss: 1.9707 - Train Acc: 34.42% - Val Acc: 55.78% - Val Loss: 1.3241 - Time: 60.21s\n",
            "Best model saved with Val Acc: 55.78%\n",
            "Epoch [7/200] - Loss: 1.9543 - Train Acc: 35.74% - Val Acc: 57.39% - Val Loss: 1.2690 - Time: 61.18s\n",
            "Best model saved with Val Acc: 57.39%\n",
            "Epoch [8/200] - Loss: 1.9165 - Train Acc: 37.78% - Val Acc: 61.41% - Val Loss: 1.1718 - Time: 60.10s\n",
            "Best model saved with Val Acc: 61.41%\n",
            "Epoch [9/200] - Loss: 1.8980 - Train Acc: 39.21% - Val Acc: 61.54% - Val Loss: 1.1885 - Time: 59.93s\n",
            "Best model saved with Val Acc: 61.54%\n",
            "Epoch [10/200] - Loss: 1.8837 - Train Acc: 39.99% - Val Acc: 62.52% - Val Loss: 1.1523 - Time: 60.90s\n",
            "Best model saved with Val Acc: 62.52%\n",
            "Epoch [11/200] - Loss: 1.8680 - Train Acc: 41.02% - Val Acc: 64.81% - Val Loss: 1.1215 - Time: 61.79s\n",
            "Best model saved with Val Acc: 64.81%\n",
            "Epoch [12/200] - Loss: 1.8661 - Train Acc: 41.33% - Val Acc: 67.67% - Val Loss: 1.0490 - Time: 60.36s\n",
            "Best model saved with Val Acc: 67.67%\n",
            "Epoch [13/200] - Loss: 1.8473 - Train Acc: 42.49% - Val Acc: 67.94% - Val Loss: 1.0348 - Time: 60.74s\n",
            "Best model saved with Val Acc: 67.94%\n",
            "Epoch [14/200] - Loss: 1.8491 - Train Acc: 42.56% - Val Acc: 69.09% - Val Loss: 1.0443 - Time: 61.66s\n",
            "Best model saved with Val Acc: 69.09%\n",
            "Epoch [15/200] - Loss: 1.8250 - Train Acc: 43.78% - Val Acc: 67.21% - Val Loss: 1.0789 - Time: 60.36s\n",
            "Epoch [16/200] - Loss: 1.8147 - Train Acc: 44.44% - Val Acc: 70.04% - Val Loss: 1.0232 - Time: 60.59s\n",
            "Best model saved with Val Acc: 70.04%\n",
            "Epoch [17/200] - Loss: 1.8015 - Train Acc: 45.02% - Val Acc: 72.20% - Val Loss: 0.9702 - Time: 61.54s\n",
            "Best model saved with Val Acc: 72.20%\n",
            "Epoch [18/200] - Loss: 1.8229 - Train Acc: 44.02% - Val Acc: 70.46% - Val Loss: 1.0393 - Time: 60.17s\n",
            "Epoch [19/200] - Loss: 1.8037 - Train Acc: 45.23% - Val Acc: 71.86% - Val Loss: 0.9957 - Time: 61.42s\n",
            "Epoch [20/200] - Loss: 1.7822 - Train Acc: 46.10% - Val Acc: 73.39% - Val Loss: 0.9397 - Time: 60.74s\n",
            "Best model saved with Val Acc: 73.39%\n",
            "Epoch [21/200] - Loss: 1.7901 - Train Acc: 46.12% - Val Acc: 72.23% - Val Loss: 1.0398 - Time: 60.69s\n",
            "Epoch [22/200] - Loss: 1.7789 - Train Acc: 46.98% - Val Acc: 73.78% - Val Loss: 0.9217 - Time: 60.89s\n",
            "Best model saved with Val Acc: 73.78%\n",
            "Epoch [23/200] - Loss: 1.7709 - Train Acc: 47.39% - Val Acc: 73.66% - Val Loss: 0.9686 - Time: 60.59s\n",
            "Epoch [24/200] - Loss: 1.7315 - Train Acc: 49.25% - Val Acc: 76.04% - Val Loss: 0.8368 - Time: 59.99s\n",
            "Best model saved with Val Acc: 76.04%\n",
            "Epoch [25/200] - Loss: 1.7633 - Train Acc: 47.70% - Val Acc: 74.55% - Val Loss: 0.9504 - Time: 61.41s\n",
            "Epoch [26/200] - Loss: 1.7492 - Train Acc: 48.79% - Val Acc: 76.34% - Val Loss: 0.9012 - Time: 61.38s\n",
            "Best model saved with Val Acc: 76.34%\n",
            "Epoch [27/200] - Loss: 1.7408 - Train Acc: 48.75% - Val Acc: 75.24% - Val Loss: 0.9621 - Time: 60.78s\n",
            "Epoch [28/200] - Loss: 1.7444 - Train Acc: 48.98% - Val Acc: 77.37% - Val Loss: 0.9141 - Time: 61.88s\n",
            "Best model saved with Val Acc: 77.37%\n",
            "Epoch [29/200] - Loss: 1.7336 - Train Acc: 49.62% - Val Acc: 74.94% - Val Loss: 0.8870 - Time: 61.41s\n",
            "Epoch [30/200] - Loss: 1.7171 - Train Acc: 50.30% - Val Acc: 76.31% - Val Loss: 0.8903 - Time: 63.57s\n",
            "Epoch [31/200] - Loss: 1.7023 - Train Acc: 51.21% - Val Acc: 75.68% - Val Loss: 0.9062 - Time: 61.78s\n",
            "Epoch [32/200] - Loss: 1.7324 - Train Acc: 49.50% - Val Acc: 76.76% - Val Loss: 0.8671 - Time: 61.40s\n",
            "Epoch [33/200] - Loss: 1.7069 - Train Acc: 50.80% - Val Acc: 77.95% - Val Loss: 0.8646 - Time: 60.10s\n",
            "Best model saved with Val Acc: 77.95%\n",
            "Epoch [34/200] - Loss: 1.7297 - Train Acc: 49.57% - Val Acc: 78.10% - Val Loss: 0.8465 - Time: 60.10s\n",
            "Best model saved with Val Acc: 78.10%\n",
            "Epoch [35/200] - Loss: 1.6964 - Train Acc: 51.09% - Val Acc: 78.63% - Val Loss: 0.8442 - Time: 60.55s\n",
            "Best model saved with Val Acc: 78.63%\n",
            "Epoch [36/200] - Loss: 1.6941 - Train Acc: 51.54% - Val Acc: 77.59% - Val Loss: 0.8904 - Time: 60.67s\n",
            "Epoch [37/200] - Loss: 1.7015 - Train Acc: 51.03% - Val Acc: 79.80% - Val Loss: 0.8339 - Time: 60.19s\n",
            "Best model saved with Val Acc: 79.80%\n",
            "Epoch [38/200] - Loss: 1.6912 - Train Acc: 51.49% - Val Acc: 79.92% - Val Loss: 0.8140 - Time: 62.65s\n",
            "Best model saved with Val Acc: 79.92%\n",
            "Epoch [39/200] - Loss: 1.6909 - Train Acc: 51.62% - Val Acc: 78.06% - Val Loss: 0.8656 - Time: 61.92s\n",
            "Epoch [40/200] - Loss: 1.6700 - Train Acc: 52.73% - Val Acc: 78.08% - Val Loss: 0.8748 - Time: 61.44s\n",
            "Epoch [41/200] - Loss: 1.6647 - Train Acc: 52.99% - Val Acc: 79.21% - Val Loss: 0.7856 - Time: 60.80s\n",
            "Epoch [42/200] - Loss: 1.6780 - Train Acc: 52.52% - Val Acc: 80.48% - Val Loss: 0.8189 - Time: 60.51s\n",
            "Best model saved with Val Acc: 80.48%\n",
            "Epoch [43/200] - Loss: 1.6593 - Train Acc: 53.23% - Val Acc: 81.28% - Val Loss: 0.8299 - Time: 62.81s\n",
            "Best model saved with Val Acc: 81.28%\n",
            "Epoch [44/200] - Loss: 1.6647 - Train Acc: 52.92% - Val Acc: 82.29% - Val Loss: 0.7550 - Time: 60.48s\n",
            "Best model saved with Val Acc: 82.29%\n",
            "Epoch [45/200] - Loss: 1.6473 - Train Acc: 54.01% - Val Acc: 80.85% - Val Loss: 0.7445 - Time: 60.99s\n",
            "Epoch [46/200] - Loss: 1.6719 - Train Acc: 52.62% - Val Acc: 80.64% - Val Loss: 0.8339 - Time: 61.26s\n",
            "Epoch [47/200] - Loss: 1.6532 - Train Acc: 53.54% - Val Acc: 81.41% - Val Loss: 0.7901 - Time: 62.04s\n",
            "Epoch [48/200] - Loss: 1.6477 - Train Acc: 53.73% - Val Acc: 82.30% - Val Loss: 0.7503 - Time: 63.57s\n",
            "Best model saved with Val Acc: 82.30%\n",
            "Epoch [49/200] - Loss: 1.6410 - Train Acc: 54.11% - Val Acc: 81.69% - Val Loss: 0.7776 - Time: 61.41s\n",
            "Epoch [50/200] - Loss: 1.6604 - Train Acc: 53.34% - Val Acc: 81.70% - Val Loss: 0.7851 - Time: 62.91s\n",
            "Epoch [51/200] - Loss: 1.6560 - Train Acc: 53.36% - Val Acc: 82.33% - Val Loss: 0.7751 - Time: 61.92s\n",
            "Best model saved with Val Acc: 82.33%\n",
            "Epoch [52/200] - Loss: 1.6235 - Train Acc: 55.19% - Val Acc: 82.89% - Val Loss: 0.7355 - Time: 61.85s\n",
            "Best model saved with Val Acc: 82.89%\n",
            "Epoch [53/200] - Loss: 1.6252 - Train Acc: 54.78% - Val Acc: 83.29% - Val Loss: 0.7311 - Time: 62.44s\n",
            "Best model saved with Val Acc: 83.29%\n",
            "Epoch [54/200] - Loss: 1.6184 - Train Acc: 55.46% - Val Acc: 83.73% - Val Loss: 0.7545 - Time: 60.97s\n",
            "Best model saved with Val Acc: 83.73%\n",
            "Epoch [55/200] - Loss: 1.6066 - Train Acc: 55.93% - Val Acc: 84.24% - Val Loss: 0.7372 - Time: 60.90s\n",
            "Best model saved with Val Acc: 84.24%\n",
            "Epoch [56/200] - Loss: 1.6251 - Train Acc: 54.79% - Val Acc: 83.83% - Val Loss: 0.7315 - Time: 60.64s\n",
            "Epoch [57/200] - Loss: 1.6182 - Train Acc: 55.51% - Val Acc: 83.64% - Val Loss: 0.7373 - Time: 60.57s\n",
            "Epoch [58/200] - Loss: 1.6207 - Train Acc: 55.21% - Val Acc: 82.81% - Val Loss: 0.7274 - Time: 60.35s\n",
            "Epoch [59/200] - Loss: 1.6020 - Train Acc: 56.01% - Val Acc: 84.52% - Val Loss: 0.7126 - Time: 61.09s\n",
            "Best model saved with Val Acc: 84.52%\n",
            "Epoch [60/200] - Loss: 1.5948 - Train Acc: 56.31% - Val Acc: 84.28% - Val Loss: 0.7150 - Time: 61.55s\n",
            "Epoch [61/200] - Loss: 1.6020 - Train Acc: 55.92% - Val Acc: 84.02% - Val Loss: 0.7183 - Time: 63.14s\n",
            "Epoch [62/200] - Loss: 1.5811 - Train Acc: 57.00% - Val Acc: 84.18% - Val Loss: 0.7769 - Time: 61.05s\n",
            "Epoch [63/200] - Loss: 1.5899 - Train Acc: 56.77% - Val Acc: 85.07% - Val Loss: 0.6650 - Time: 61.33s\n",
            "Best model saved with Val Acc: 85.07%\n",
            "Epoch [64/200] - Loss: 1.5997 - Train Acc: 56.11% - Val Acc: 84.22% - Val Loss: 0.7716 - Time: 60.62s\n",
            "Epoch [65/200] - Loss: 1.6021 - Train Acc: 56.10% - Val Acc: 86.13% - Val Loss: 0.6975 - Time: 59.69s\n",
            "Best model saved with Val Acc: 86.13%\n",
            "Epoch [66/200] - Loss: 1.5982 - Train Acc: 56.06% - Val Acc: 85.10% - Val Loss: 0.7445 - Time: 60.37s\n",
            "Epoch [67/200] - Loss: 1.5908 - Train Acc: 56.27% - Val Acc: 85.17% - Val Loss: 0.7385 - Time: 61.57s\n",
            "Epoch [68/200] - Loss: 1.5836 - Train Acc: 56.98% - Val Acc: 85.20% - Val Loss: 0.7180 - Time: 59.66s\n",
            "Epoch [69/200] - Loss: 1.5988 - Train Acc: 56.09% - Val Acc: 84.40% - Val Loss: 0.7410 - Time: 59.88s\n",
            "Epoch [70/200] - Loss: 1.5853 - Train Acc: 57.00% - Val Acc: 85.79% - Val Loss: 0.7037 - Time: 60.73s\n",
            "Epoch [71/200] - Loss: 1.5833 - Train Acc: 56.97% - Val Acc: 84.89% - Val Loss: 0.6842 - Time: 60.40s\n",
            "Epoch [72/200] - Loss: 1.5797 - Train Acc: 56.86% - Val Acc: 84.93% - Val Loss: 0.7110 - Time: 60.67s\n",
            "Epoch [73/200] - Loss: 1.5812 - Train Acc: 57.40% - Val Acc: 85.73% - Val Loss: 0.6754 - Time: 59.96s\n",
            "Epoch [74/200] - Loss: 1.5733 - Train Acc: 57.51% - Val Acc: 86.25% - Val Loss: 0.7093 - Time: 60.53s\n",
            "Best model saved with Val Acc: 86.25%\n",
            "Epoch [75/200] - Loss: 1.5469 - Train Acc: 58.55% - Val Acc: 86.79% - Val Loss: 0.6424 - Time: 61.78s\n",
            "Best model saved with Val Acc: 86.79%\n",
            "Epoch [76/200] - Loss: 1.5681 - Train Acc: 57.74% - Val Acc: 85.08% - Val Loss: 0.6680 - Time: 60.83s\n",
            "Epoch [77/200] - Loss: 1.5470 - Train Acc: 58.56% - Val Acc: 86.24% - Val Loss: 0.6607 - Time: 60.76s\n",
            "Epoch [78/200] - Loss: 1.5561 - Train Acc: 57.85% - Val Acc: 85.89% - Val Loss: 0.6932 - Time: 61.83s\n",
            "Epoch [79/200] - Loss: 1.5638 - Train Acc: 57.77% - Val Acc: 85.52% - Val Loss: 0.7330 - Time: 59.18s\n",
            "Epoch [80/200] - Loss: 1.5691 - Train Acc: 57.54% - Val Acc: 86.55% - Val Loss: 0.6924 - Time: 60.16s\n",
            "Epoch [81/200] - Loss: 1.5320 - Train Acc: 59.67% - Val Acc: 84.92% - Val Loss: 0.6816 - Time: 64.63s\n",
            "Epoch [82/200] - Loss: 1.5561 - Train Acc: 57.87% - Val Acc: 86.30% - Val Loss: 0.6447 - Time: 61.28s\n",
            "Epoch [83/200] - Loss: 1.5627 - Train Acc: 57.54% - Val Acc: 86.96% - Val Loss: 0.6870 - Time: 61.65s\n",
            "Best model saved with Val Acc: 86.96%\n",
            "Epoch [84/200] - Loss: 1.5486 - Train Acc: 58.63% - Val Acc: 85.09% - Val Loss: 0.6700 - Time: 61.58s\n",
            "Epoch [85/200] - Loss: 1.5401 - Train Acc: 59.14% - Val Acc: 86.76% - Val Loss: 0.6805 - Time: 61.21s\n",
            "Epoch [86/200] - Loss: 1.5453 - Train Acc: 58.42% - Val Acc: 87.17% - Val Loss: 0.6687 - Time: 61.83s\n",
            "Best model saved with Val Acc: 87.17%\n",
            "Epoch [87/200] - Loss: 1.5213 - Train Acc: 59.62% - Val Acc: 86.97% - Val Loss: 0.6485 - Time: 61.00s\n",
            "Epoch [88/200] - Loss: 1.5219 - Train Acc: 59.95% - Val Acc: 88.05% - Val Loss: 0.6037 - Time: 59.58s\n",
            "Best model saved with Val Acc: 88.05%\n",
            "Epoch [89/200] - Loss: 1.5147 - Train Acc: 60.00% - Val Acc: 87.00% - Val Loss: 0.5961 - Time: 59.88s\n",
            "Epoch [90/200] - Loss: 1.5339 - Train Acc: 59.44% - Val Acc: 87.50% - Val Loss: 0.6244 - Time: 59.85s\n",
            "Epoch [91/200] - Loss: 1.5230 - Train Acc: 59.85% - Val Acc: 87.24% - Val Loss: 0.6088 - Time: 60.10s\n",
            "Epoch [92/200] - Loss: 1.5212 - Train Acc: 59.78% - Val Acc: 87.92% - Val Loss: 0.6108 - Time: 60.55s\n",
            "Epoch [93/200] - Loss: 1.5467 - Train Acc: 58.52% - Val Acc: 87.80% - Val Loss: 0.6282 - Time: 60.48s\n",
            "Epoch [94/200] - Loss: 1.5147 - Train Acc: 59.88% - Val Acc: 86.87% - Val Loss: 0.6794 - Time: 61.14s\n",
            "Epoch [95/200] - Loss: 1.4911 - Train Acc: 61.38% - Val Acc: 87.55% - Val Loss: 0.5846 - Time: 61.19s\n",
            "Epoch [96/200] - Loss: 1.5342 - Train Acc: 59.11% - Val Acc: 88.01% - Val Loss: 0.6337 - Time: 60.60s\n",
            "Epoch [97/200] - Loss: 1.5143 - Train Acc: 59.99% - Val Acc: 87.31% - Val Loss: 0.6372 - Time: 61.15s\n",
            "Epoch [98/200] - Loss: 1.5366 - Train Acc: 58.76% - Val Acc: 88.05% - Val Loss: 0.6514 - Time: 60.29s\n",
            "Epoch [99/200] - Loss: 1.5361 - Train Acc: 59.08% - Val Acc: 88.44% - Val Loss: 0.6112 - Time: 59.95s\n",
            "Best model saved with Val Acc: 88.44%\n",
            "Epoch [100/200] - Loss: 1.5270 - Train Acc: 59.38% - Val Acc: 88.07% - Val Loss: 0.6295 - Time: 60.51s\n",
            "Epoch [101/200] - Loss: 1.5215 - Train Acc: 59.74% - Val Acc: 87.34% - Val Loss: 0.6050 - Time: 60.60s\n",
            "Epoch [102/200] - Loss: 1.5000 - Train Acc: 60.65% - Val Acc: 88.11% - Val Loss: 0.6379 - Time: 61.15s\n",
            "Epoch [103/200] - Loss: 1.5114 - Train Acc: 59.92% - Val Acc: 88.34% - Val Loss: 0.6570 - Time: 60.80s\n",
            "Epoch [104/200] - Loss: 1.5216 - Train Acc: 59.85% - Val Acc: 88.48% - Val Loss: 0.5788 - Time: 61.20s\n",
            "Best model saved with Val Acc: 88.48%\n",
            "Epoch [105/200] - Loss: 1.5094 - Train Acc: 60.56% - Val Acc: 88.47% - Val Loss: 0.6316 - Time: 61.73s\n",
            "Epoch [106/200] - Loss: 1.5084 - Train Acc: 60.23% - Val Acc: 88.28% - Val Loss: 0.6420 - Time: 61.58s\n",
            "Epoch [107/200] - Loss: 1.5138 - Train Acc: 59.98% - Val Acc: 88.09% - Val Loss: 0.6018 - Time: 60.64s\n",
            "Epoch [108/200] - Loss: 1.4875 - Train Acc: 61.23% - Val Acc: 88.18% - Val Loss: 0.6593 - Time: 60.90s\n",
            "Epoch [109/200] - Loss: 1.5257 - Train Acc: 59.46% - Val Acc: 88.60% - Val Loss: 0.5896 - Time: 61.88s\n",
            "Best model saved with Val Acc: 88.60%\n",
            "Epoch [110/200] - Loss: 1.5263 - Train Acc: 59.36% - Val Acc: 87.68% - Val Loss: 0.6344 - Time: 60.09s\n",
            "Epoch [111/200] - Loss: 1.5120 - Train Acc: 59.92% - Val Acc: 89.23% - Val Loss: 0.5938 - Time: 61.61s\n",
            "Best model saved with Val Acc: 89.23%\n",
            "Epoch [112/200] - Loss: 1.5015 - Train Acc: 60.45% - Val Acc: 88.96% - Val Loss: 0.6263 - Time: 61.94s\n",
            "Epoch [113/200] - Loss: 1.4934 - Train Acc: 61.10% - Val Acc: 89.51% - Val Loss: 0.5910 - Time: 60.76s\n",
            "Best model saved with Val Acc: 89.51%\n",
            "Epoch [114/200] - Loss: 1.4766 - Train Acc: 61.79% - Val Acc: 89.06% - Val Loss: 0.6084 - Time: 61.45s\n",
            "Epoch [115/200] - Loss: 1.4688 - Train Acc: 62.21% - Val Acc: 89.10% - Val Loss: 0.6101 - Time: 60.63s\n",
            "Epoch [116/200] - Loss: 1.5064 - Train Acc: 60.27% - Val Acc: 89.30% - Val Loss: 0.6203 - Time: 59.76s\n",
            "Epoch [117/200] - Loss: 1.4713 - Train Acc: 62.07% - Val Acc: 89.22% - Val Loss: 0.5876 - Time: 59.99s\n",
            "Epoch [118/200] - Loss: 1.5015 - Train Acc: 60.81% - Val Acc: 89.28% - Val Loss: 0.5787 - Time: 60.20s\n",
            "Epoch [119/200] - Loss: 1.5159 - Train Acc: 59.80% - Val Acc: 88.77% - Val Loss: 0.6383 - Time: 60.36s\n",
            "Epoch [120/200] - Loss: 1.4829 - Train Acc: 61.29% - Val Acc: 89.48% - Val Loss: 0.6131 - Time: 60.57s\n",
            "Epoch [121/200] - Loss: 1.4937 - Train Acc: 60.68% - Val Acc: 89.05% - Val Loss: 0.5859 - Time: 60.53s\n",
            "Epoch [122/200] - Loss: 1.4692 - Train Acc: 61.94% - Val Acc: 88.81% - Val Loss: 0.6471 - Time: 59.44s\n",
            "Epoch [123/200] - Loss: 1.4696 - Train Acc: 62.26% - Val Acc: 88.99% - Val Loss: 0.6233 - Time: 60.06s\n",
            "Epoch [124/200] - Loss: 1.4719 - Train Acc: 61.61% - Val Acc: 89.62% - Val Loss: 0.5315 - Time: 60.55s\n",
            "Best model saved with Val Acc: 89.62%\n",
            "Epoch [125/200] - Loss: 1.4832 - Train Acc: 61.36% - Val Acc: 89.55% - Val Loss: 0.6119 - Time: 60.78s\n",
            "Epoch [126/200] - Loss: 1.4639 - Train Acc: 61.90% - Val Acc: 89.53% - Val Loss: 0.5795 - Time: 60.49s\n",
            "Epoch [127/200] - Loss: 1.4621 - Train Acc: 62.11% - Val Acc: 89.59% - Val Loss: 0.5757 - Time: 60.09s\n",
            "Epoch [128/200] - Loss: 1.4765 - Train Acc: 61.87% - Val Acc: 89.70% - Val Loss: 0.5596 - Time: 59.32s\n",
            "Best model saved with Val Acc: 89.70%\n",
            "Epoch [129/200] - Loss: 1.4740 - Train Acc: 61.83% - Val Acc: 89.51% - Val Loss: 0.5922 - Time: 60.05s\n",
            "Epoch [130/200] - Loss: 1.4747 - Train Acc: 62.01% - Val Acc: 89.50% - Val Loss: 0.5543 - Time: 60.70s\n",
            "Epoch [131/200] - Loss: 1.4590 - Train Acc: 62.74% - Val Acc: 89.58% - Val Loss: 0.5719 - Time: 60.73s\n",
            "Epoch [132/200] - Loss: 1.4777 - Train Acc: 61.60% - Val Acc: 90.20% - Val Loss: 0.5586 - Time: 61.37s\n",
            "Best model saved with Val Acc: 90.20%\n",
            "Epoch [133/200] - Loss: 1.4701 - Train Acc: 62.26% - Val Acc: 90.09% - Val Loss: 0.5229 - Time: 60.48s\n",
            "Epoch [134/200] - Loss: 1.4579 - Train Acc: 62.68% - Val Acc: 90.19% - Val Loss: 0.5837 - Time: 60.36s\n",
            "Epoch [135/200] - Loss: 1.4658 - Train Acc: 62.17% - Val Acc: 89.80% - Val Loss: 0.5718 - Time: 60.84s\n",
            "Epoch [136/200] - Loss: 1.4518 - Train Acc: 63.03% - Val Acc: 89.71% - Val Loss: 0.5581 - Time: 60.58s\n",
            "Epoch [137/200] - Loss: 1.4602 - Train Acc: 62.71% - Val Acc: 90.30% - Val Loss: 0.5589 - Time: 60.39s\n",
            "Best model saved with Val Acc: 90.30%\n",
            "Epoch [138/200] - Loss: 1.4425 - Train Acc: 63.28% - Val Acc: 90.15% - Val Loss: 0.5793 - Time: 60.68s\n",
            "Epoch [139/200] - Loss: 1.4360 - Train Acc: 63.61% - Val Acc: 89.88% - Val Loss: 0.5992 - Time: 61.11s\n",
            "Epoch [140/200] - Loss: 1.4703 - Train Acc: 62.02% - Val Acc: 90.51% - Val Loss: 0.5719 - Time: 61.25s\n",
            "Best model saved with Val Acc: 90.51%\n",
            "Epoch [141/200] - Loss: 1.4426 - Train Acc: 62.91% - Val Acc: 90.00% - Val Loss: 0.5654 - Time: 62.13s\n",
            "Epoch [142/200] - Loss: 1.4808 - Train Acc: 61.29% - Val Acc: 90.03% - Val Loss: 0.5649 - Time: 60.18s\n",
            "Epoch [143/200] - Loss: 1.4647 - Train Acc: 62.44% - Val Acc: 90.48% - Val Loss: 0.5787 - Time: 60.46s\n",
            "Epoch [144/200] - Loss: 1.4475 - Train Acc: 63.19% - Val Acc: 90.60% - Val Loss: 0.5338 - Time: 60.87s\n",
            "Best model saved with Val Acc: 90.60%\n",
            "Epoch [145/200] - Loss: 1.4652 - Train Acc: 61.89% - Val Acc: 90.34% - Val Loss: 0.5699 - Time: 60.22s\n",
            "Epoch [146/200] - Loss: 1.4607 - Train Acc: 62.36% - Val Acc: 90.67% - Val Loss: 0.5260 - Time: 59.64s\n",
            "Best model saved with Val Acc: 90.67%\n",
            "Epoch [147/200] - Loss: 1.4463 - Train Acc: 62.93% - Val Acc: 90.29% - Val Loss: 0.5782 - Time: 61.09s\n",
            "Epoch [148/200] - Loss: 1.4660 - Train Acc: 62.37% - Val Acc: 90.76% - Val Loss: 0.5609 - Time: 59.19s\n",
            "Best model saved with Val Acc: 90.76%\n",
            "Epoch [149/200] - Loss: 1.4567 - Train Acc: 62.38% - Val Acc: 90.78% - Val Loss: 0.5430 - Time: 60.28s\n",
            "Best model saved with Val Acc: 90.78%\n",
            "Epoch [150/200] - Loss: 1.4682 - Train Acc: 62.02% - Val Acc: 90.76% - Val Loss: 0.5442 - Time: 59.71s\n",
            "Epoch [151/200] - Loss: 1.4553 - Train Acc: 62.42% - Val Acc: 90.28% - Val Loss: 0.5497 - Time: 59.64s\n",
            "Epoch [152/200] - Loss: 1.4614 - Train Acc: 62.43% - Val Acc: 90.65% - Val Loss: 0.5702 - Time: 59.91s\n",
            "Epoch [153/200] - Loss: 1.4548 - Train Acc: 62.73% - Val Acc: 90.79% - Val Loss: 0.5641 - Time: 60.73s\n",
            "Best model saved with Val Acc: 90.79%\n",
            "Epoch [154/200] - Loss: 1.4584 - Train Acc: 62.62% - Val Acc: 90.72% - Val Loss: 0.5262 - Time: 59.92s\n",
            "Epoch [155/200] - Loss: 1.4447 - Train Acc: 63.04% - Val Acc: 90.67% - Val Loss: 0.5326 - Time: 60.27s\n",
            "Epoch [156/200] - Loss: 1.4369 - Train Acc: 63.48% - Val Acc: 90.71% - Val Loss: 0.5181 - Time: 60.45s\n",
            "Epoch [157/200] - Loss: 1.4293 - Train Acc: 63.87% - Val Acc: 90.63% - Val Loss: 0.5532 - Time: 61.63s\n",
            "Epoch [158/200] - Loss: 1.4238 - Train Acc: 63.86% - Val Acc: 91.01% - Val Loss: 0.5233 - Time: 62.25s\n",
            "Best model saved with Val Acc: 91.01%\n",
            "Epoch [159/200] - Loss: 1.4232 - Train Acc: 64.10% - Val Acc: 90.74% - Val Loss: 0.5585 - Time: 64.31s\n",
            "Epoch [160/200] - Loss: 1.4380 - Train Acc: 63.42% - Val Acc: 90.79% - Val Loss: 0.5535 - Time: 61.30s\n",
            "Epoch [161/200] - Loss: 1.4748 - Train Acc: 61.40% - Val Acc: 90.91% - Val Loss: 0.5633 - Time: 60.96s\n",
            "Epoch [162/200] - Loss: 1.4080 - Train Acc: 64.66% - Val Acc: 91.11% - Val Loss: 0.5268 - Time: 60.32s\n",
            "Best model saved with Val Acc: 91.11%\n",
            "Epoch [163/200] - Loss: 1.4601 - Train Acc: 62.19% - Val Acc: 90.89% - Val Loss: 0.5570 - Time: 60.24s\n",
            "Epoch [164/200] - Loss: 1.4264 - Train Acc: 63.95% - Val Acc: 90.95% - Val Loss: 0.5420 - Time: 60.05s\n",
            "Epoch [165/200] - Loss: 1.4327 - Train Acc: 63.51% - Val Acc: 90.99% - Val Loss: 0.5533 - Time: 59.94s\n",
            "Epoch [166/200] - Loss: 1.4378 - Train Acc: 63.35% - Val Acc: 91.06% - Val Loss: 0.5501 - Time: 60.26s\n",
            "Epoch [167/200] - Loss: 1.4234 - Train Acc: 63.81% - Val Acc: 90.84% - Val Loss: 0.5578 - Time: 60.55s\n",
            "Epoch [168/200] - Loss: 1.4105 - Train Acc: 64.51% - Val Acc: 90.91% - Val Loss: 0.5495 - Time: 60.58s\n",
            "Epoch [169/200] - Loss: 1.4390 - Train Acc: 63.30% - Val Acc: 91.21% - Val Loss: 0.5134 - Time: 60.07s\n",
            "Best model saved with Val Acc: 91.21%\n",
            "Epoch [170/200] - Loss: 1.4234 - Train Acc: 64.16% - Val Acc: 90.98% - Val Loss: 0.5659 - Time: 60.28s\n",
            "Epoch [171/200] - Loss: 1.4232 - Train Acc: 63.79% - Val Acc: 91.14% - Val Loss: 0.5346 - Time: 59.72s\n",
            "Epoch [172/200] - Loss: 1.4421 - Train Acc: 62.79% - Val Acc: 90.94% - Val Loss: 0.5609 - Time: 59.81s\n",
            "Epoch [173/200] - Loss: 1.4165 - Train Acc: 64.21% - Val Acc: 91.16% - Val Loss: 0.5415 - Time: 60.72s\n",
            "Epoch [174/200] - Loss: 1.4188 - Train Acc: 64.11% - Val Acc: 91.21% - Val Loss: 0.5521 - Time: 61.27s\n",
            "Epoch [175/200] - Loss: 1.4187 - Train Acc: 64.44% - Val Acc: 91.33% - Val Loss: 0.5254 - Time: 62.07s\n",
            "Best model saved with Val Acc: 91.33%\n",
            "Epoch [176/200] - Loss: 1.4307 - Train Acc: 63.71% - Val Acc: 90.96% - Val Loss: 0.5761 - Time: 62.21s\n",
            "Epoch [177/200] - Loss: 1.4401 - Train Acc: 62.84% - Val Acc: 91.14% - Val Loss: 0.5683 - Time: 61.42s\n",
            "Epoch [178/200] - Loss: 1.4277 - Train Acc: 63.67% - Val Acc: 91.11% - Val Loss: 0.5359 - Time: 62.31s\n",
            "Epoch [179/200] - Loss: 1.4062 - Train Acc: 64.45% - Val Acc: 91.40% - Val Loss: 0.5283 - Time: 61.38s\n",
            "Best model saved with Val Acc: 91.40%\n",
            "Epoch [180/200] - Loss: 1.4120 - Train Acc: 64.42% - Val Acc: 90.99% - Val Loss: 0.5829 - Time: 61.52s\n",
            "Epoch [181/200] - Loss: 1.4377 - Train Acc: 63.14% - Val Acc: 91.17% - Val Loss: 0.5669 - Time: 60.58s\n",
            "Epoch [182/200] - Loss: 1.4232 - Train Acc: 63.86% - Val Acc: 91.26% - Val Loss: 0.5280 - Time: 61.16s\n",
            "Epoch [183/200] - Loss: 1.4033 - Train Acc: 64.78% - Val Acc: 91.47% - Val Loss: 0.5190 - Time: 63.93s\n",
            "Best model saved with Val Acc: 91.47%\n",
            "Epoch [184/200] - Loss: 1.4228 - Train Acc: 64.09% - Val Acc: 91.62% - Val Loss: 0.4885 - Time: 61.49s\n",
            "Best model saved with Val Acc: 91.62%\n",
            "Epoch [185/200] - Loss: 1.4278 - Train Acc: 63.89% - Val Acc: 91.35% - Val Loss: 0.5227 - Time: 61.45s\n",
            "Epoch [186/200] - Loss: 1.4354 - Train Acc: 63.18% - Val Acc: 91.04% - Val Loss: 0.5680 - Time: 61.40s\n",
            "Epoch [187/200] - Loss: 1.4419 - Train Acc: 62.92% - Val Acc: 91.53% - Val Loss: 0.5037 - Time: 61.57s\n",
            "Epoch [188/200] - Loss: 1.4150 - Train Acc: 64.29% - Val Acc: 91.26% - Val Loss: 0.5376 - Time: 63.29s\n",
            "Epoch [189/200] - Loss: 1.4193 - Train Acc: 63.71% - Val Acc: 91.38% - Val Loss: 0.5372 - Time: 61.01s\n",
            "Epoch [190/200] - Loss: 1.4210 - Train Acc: 64.25% - Val Acc: 91.34% - Val Loss: 0.5400 - Time: 61.52s\n",
            "Epoch [191/200] - Loss: 1.3762 - Train Acc: 66.22% - Val Acc: 91.51% - Val Loss: 0.5206 - Time: 61.27s\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingWarmRestarts\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# ----------------------------\n",
        "# Set Seed for Reproducibility\n",
        "# ----------------------------\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# ----------------------------\n",
        "# Residual Block with Squeeze-and-Excitation (SE) and DropBlock\n",
        "# ----------------------------\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual block with Squeeze-and-Excitation and DropBlock.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1, drop_prob=0.0):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
        "                               kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
        "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.se = SqueezeExcitation(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride !=1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        self.dropblock = DropBlock(drop_prob) if drop_prob > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out = self.se(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropblock(out)\n",
        "        return out\n",
        "\n",
        "# ----------------------------\n",
        "# Squeeze-and-Excitation Block\n",
        "# ----------------------------\n",
        "class SqueezeExcitation(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SqueezeExcitation, self).__init__()\n",
        "        self.fc1 = nn.Linear(channel, channel // reduction, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(channel // reduction, channel, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = F.adaptive_avg_pool2d(x, 1).view(b, c)\n",
        "        y = self.fc1(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.sigmoid(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "# ----------------------------\n",
        "# DropBlock Regularization\n",
        "# ----------------------------\n",
        "class DropBlock(nn.Module):\n",
        "    def __init__(self, drop_prob=0.3, block_size=7):\n",
        "        super(DropBlock, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training or self.drop_prob == 0.0:\n",
        "            return x\n",
        "        else:\n",
        "            # Get gamma\n",
        "            gamma = self._compute_gamma(x)\n",
        "            # Sample mask\n",
        "            mask = (torch.rand(x.shape[:2], device=x.device) < gamma).float()\n",
        "            # Compute block mask\n",
        "            mask = F.max_pool2d(mask.unsqueeze(1), kernel_size=self.block_size,\n",
        "                                stride=1, padding=self.block_size//2)\n",
        "            mask = 1 - mask.squeeze(1)\n",
        "            # Apply mask\n",
        "            out = x * mask.unsqueeze(2).unsqueeze(3)\n",
        "            out = out * (mask.numel() / mask.sum())\n",
        "            return out\n",
        "\n",
        "    def _compute_gamma(self, x):\n",
        "        b, c, h, w = x.size()\n",
        "        return self.drop_prob * (h * w) / (self.block_size **2) / ((h - self.block_size +1) * (w - self.block_size +1))\n",
        "\n",
        "# ----------------------------\n",
        "# Global Feature Extractor\n",
        "# ----------------------------\n",
        "class GlobalFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracts global features from the input image with residual connections.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, num_blocks=4, drop_prob=0.3):\n",
        "        super(GlobalFeatureExtractor, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(num_blocks):\n",
        "            stride = 1 if i ==0 else 2\n",
        "            layers.append(ResidualBlock(in_channels if i ==0 else out_channels, out_channels, stride, drop_prob))\n",
        "            in_channels = out_channels\n",
        "        self.layer = nn.Sequential(*layers)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x  # [B, out_channels]\n",
        "\n",
        "# ----------------------------\n",
        "# Local Feature Extractor\n",
        "# ----------------------------\n",
        "class LocalFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracts local features from the input image with residual connections.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, num_blocks=4, drop_prob=0.3):\n",
        "        super(LocalFeatureExtractor, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(num_blocks):\n",
        "            stride = 1 if i ==0 else 2\n",
        "            layers.append(ResidualBlock(in_channels if i ==0 else out_channels, out_channels, stride, drop_prob))\n",
        "            in_channels = out_channels\n",
        "        self.layer = nn.Sequential(*layers)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((2,2))  # Retains some spatial information\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer(x)\n",
        "        x = self.pool(x)\n",
        "        return x  # [B, out_channels, 2, 2]\n",
        "\n",
        "# ----------------------------\n",
        "# Enhanced Adaptive Attention Module with Multi-Head Attention\n",
        "# ----------------------------\n",
        "class EnhancedAdaptiveAttentionModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Enhanced Adaptive Attention Module with Multi-Head Attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, global_dim, local_dim, num_heads=4):\n",
        "        super(EnhancedAdaptiveAttentionModule, self).__init__()\n",
        "        self.query = nn.Linear(global_dim + local_dim, global_dim)\n",
        "        self.key = nn.Linear(global_dim + local_dim, global_dim)\n",
        "        self.value = nn.Linear(global_dim + local_dim, global_dim)\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim=global_dim, num_heads=num_heads, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(global_dim, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 2),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, global_feat, local_feat):\n",
        "        # global_feat: [B, global_dim]\n",
        "        # local_feat: [B, local_dim, H, W]\n",
        "        local_feat_mean = torch.mean(local_feat, dim=[2,3])  # [B, local_dim]\n",
        "        combined = torch.cat((global_feat, local_feat_mean), dim=1)  # [B, global_dim + local_dim]\n",
        "\n",
        "        query = self.query(combined).unsqueeze(1)  # [B,1,global_dim]\n",
        "        key = self.key(combined).unsqueeze(1)      # [B,1,global_dim]\n",
        "        value = self.value(combined).unsqueeze(1)  # [B,1,global_dim]\n",
        "\n",
        "        attn_output, _ = self.multihead_attn(query, key, value)  # [B,1,global_dim]\n",
        "        attn_output = attn_output.squeeze(1)  # [B, global_dim]\n",
        "\n",
        "        weights = self.fc(attn_output)  # [B,2]\n",
        "\n",
        "        global_weight = weights[:,0].unsqueeze(1)  # [B,1]\n",
        "        local_weight = weights[:,1].unsqueeze(1)   # [B,1]\n",
        "\n",
        "        return global_weight, local_weight  # Each [B,1]\n",
        "\n",
        "# ----------------------------\n",
        "# Dynamic Holistic Perception Network\n",
        "# ----------------------------\n",
        "class DynamicHolisticPerceptionNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Dynamic Holistic Perception Network combining global and local features with residual connections.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(DynamicHolisticPerceptionNetwork, self).__init__()\n",
        "        # Initial Convolutional Layers\n",
        "        self.initial_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, kernel_size=3, padding=1),  # [B,128,32,32]\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # [B,128,16,16]\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),  # [B,128,16,16]\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)   # [B,128,8,8]\n",
        "        )\n",
        "\n",
        "        # Feature Extractors with Residual Blocks\n",
        "        self.global_extractor = GlobalFeatureExtractor(in_channels=128, out_channels=256, num_blocks=4, drop_prob=0.3)  # Output: [B,256]\n",
        "        self.local_extractor = LocalFeatureExtractor(in_channels=128, out_channels=256, num_blocks=4, drop_prob=0.3)    # Output: [B,256,2,2]\n",
        "\n",
        "        # Enhanced Adaptive Attention Module\n",
        "        self.adaptive_attention = EnhancedAdaptiveAttentionModule(global_dim=256, local_dim=256, num_heads=4)\n",
        "\n",
        "        # Fusion and Classification Layers\n",
        "        self.fusion_fc = nn.Sequential(\n",
        "            nn.Linear(256 + 256, 512),  # [B,512]\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial_conv(x)  # [B,128,8,8]\n",
        "        global_feat = self.global_extractor(x)  # [B,256]\n",
        "        local_feat = self.local_extractor(x)    # [B,256,2,2]\n",
        "\n",
        "        global_weight, local_weight = self.adaptive_attention(global_feat, local_feat)  # Each [B,1]\n",
        "\n",
        "        # Weight the features\n",
        "        global_feat_weighted = global_feat * global_weight  # [B,256]\n",
        "        local_feat_pooled = F.adaptive_avg_pool2d(local_feat, (1,1)).view(local_feat.size(0), -1)  # [B,256]\n",
        "        local_feat_weighted = local_feat_pooled * local_weight  # [B,256]\n",
        "\n",
        "        # Concatenate weighted features\n",
        "        fused_feat = torch.cat((global_feat_weighted, local_feat_weighted), dim=1)  # [B,512]\n",
        "\n",
        "        # Classification\n",
        "        out = self.fusion_fc(fused_feat)  # [B, num_classes]\n",
        "        return out\n",
        "\n",
        "# ----------------------------\n",
        "# Mixup and CutMix Functions\n",
        "# ----------------------------\n",
        "def rand_bbox(size, lam):\n",
        "    '''Generates a random bounding box for CutMix.'''\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "\n",
        "    # uniform\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def cutmix_data(x, y, alpha=1.0):\n",
        "    '''Applies CutMix to the batch.'''\n",
        "    if alpha > 0.:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1.\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "\n",
        "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
        "    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
        "\n",
        "    # Adjust lambda to exactly match pixel ratio\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n",
        "    return x, y, y[index], lam\n",
        "\n",
        "def mixup_data(x, y, alpha=1.0):\n",
        "    '''Applies MixUp to the batch.'''\n",
        "    if alpha > 0.:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1.\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# ----------------------------\n",
        "# Label Smoothing Cross Entropy Loss\n",
        "# ----------------------------\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1.0 - smoothing\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        log_probs = F.log_softmax(x, dim=-1)\n",
        "        true_dist = torch.zeros_like(log_probs)\n",
        "        true_dist.fill_(self.smoothing / (x.size(1) -1))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluation Function\n",
        "# ----------------------------\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the given dataloader.\n",
        "    Returns accuracy, confusion matrix, and average loss.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_losses = []\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)  # [B, num_classes]\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)  # [B]\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_losses.append(loss.item() * inputs.size(0))\n",
        "    acc = accuracy_score(all_labels, all_preds) * 100\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    avg_loss = sum(all_losses) / len(dataloader.dataset)\n",
        "    return acc, cm, avg_loss\n",
        "\n",
        "# ----------------------------\n",
        "# Visualization Functions\n",
        "# ----------------------------\n",
        "def plot_metrics(train_losses, val_accuracies, val_losses, train_accuracies):\n",
        "    \"\"\"\n",
        "    Plots training and validation loss and accuracy over epochs.\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(train_losses)+1)\n",
        "\n",
        "    plt.figure(figsize=(14,5))\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
        "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n",
        "    plt.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(cm, classes):\n",
        "    \"\"\"\n",
        "    Plots the confusion matrix.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=classes, yticklabels=classes)\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# ----------------------------\n",
        "# Main Training and Evaluation Pipeline\n",
        "# ----------------------------\n",
        "def main():\n",
        "    # ----------------------------\n",
        "    # Device Configuration\n",
        "    # ----------------------------\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Hyperparameters\n",
        "    # ----------------------------\n",
        "    num_epochs = 200  # Increased epochs for better convergence\n",
        "    batch_size = 128\n",
        "    learning_rate = 0.001\n",
        "    num_classes = 10  # For CIFAR-10\n",
        "    patience = 30  # For early stopping\n",
        "    alpha = 1.0  # Mixup and CutMix alpha\n",
        "    smoothing = 0.1  # Label smoothing\n",
        "\n",
        "    # ----------------------------\n",
        "    # Data Transformations with Advanced Augmentation\n",
        "    # ----------------------------\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "        transforms.RandomErasing(p=0.1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random')\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    # ----------------------------\n",
        "    # Load Datasets\n",
        "    # ----------------------------\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                           download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Initialize Model, Loss, Optimizer, Scheduler\n",
        "    # ----------------------------\n",
        "    model = DynamicHolisticPerceptionNetwork(num_classes=num_classes).to(device)\n",
        "    print(\"Model initialized.\")\n",
        "    print(model)\n",
        "\n",
        "    criterion = LabelSmoothingCrossEntropy(smoothing=smoothing)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(trainloader),\n",
        "                           epochs=num_epochs, anneal_strategy='cos')\n",
        "\n",
        "    # ----------------------------\n",
        "    # Initialize Tracking Variables\n",
        "    # ----------------------------\n",
        "    best_val_acc = 0.0\n",
        "    trigger_times = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    # ----------------------------\n",
        "    # Class Names for CIFAR-10\n",
        "    # ----------------------------\n",
        "    classes = ['Airplane','Automobile','Bird','Cat','Deer',\n",
        "               'Dog','Frog','Horse','Ship','Truck']\n",
        "\n",
        "    # ----------------------------\n",
        "    # Delete existing best_model.pth to avoid loading incompatible weights\n",
        "    # ----------------------------\n",
        "    if os.path.exists('best_model.pth'):\n",
        "        os.remove('best_model.pth')\n",
        "        print(\"Existing 'best_model.pth' removed.\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Training Loop with Mixup, CutMix, Label Smoothing, and Early Stopping\n",
        "    # ----------------------------\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Apply CutMix with probability 0.5\n",
        "            r = np.random.rand()\n",
        "            if r < 0.5:\n",
        "                inputs, targets_a, targets_b, lam = cutmix_data(inputs, labels, alpha=alpha)\n",
        "                outputs = model(inputs)\n",
        "                loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
        "            else:\n",
        "                # Apply MixUp\n",
        "                inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, alpha=alpha)\n",
        "                outputs = model(inputs)\n",
        "                loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Gradient Clipping\n",
        "            optimizer.step()\n",
        "            scheduler.step()  # Updated: Removed epoch parameter\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            # For accuracy, take the prediction and compare to targets_a and targets_b\n",
        "            correct += (lam * predicted.eq(targets_a).sum().item() + (1 - lam) * predicted.eq(targets_b).sum().item())\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = 100. * correct / total\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracies.append(epoch_acc)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        # Validation\n",
        "        val_acc, val_cm, val_loss = evaluate_model(model, testloader, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
        "              f\"Loss: {epoch_loss:.4f} - \"\n",
        "              f\"Train Acc: {epoch_acc:.2f}% - \"\n",
        "              f\"Val Acc: {val_acc:.2f}% - \"\n",
        "              f\"Val Loss: {val_loss:.4f} - \"\n",
        "              f\"Time: {elapsed_time:.2f}s\")\n",
        "\n",
        "        # Early Stopping and Saving Best Model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            trigger_times = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(f\"Best model saved with Val Acc: {best_val_acc:.2f}%\")\n",
        "        else:\n",
        "            trigger_times += 1\n",
        "            if trigger_times >= patience:\n",
        "                print(\"Early stopping triggered!\")\n",
        "                break\n",
        "\n",
        "    # ----------------------------\n",
        "    # Final Evaluation\n",
        "    # ----------------------------\n",
        "    print(\"\\nTraining Completed!\")\n",
        "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "    # Load the best model\n",
        "    if os.path.exists('best_model.pth'):\n",
        "        try:\n",
        "            state_dict = torch.load('best_model.pth', map_location=device)\n",
        "            model.load_state_dict(state_dict)\n",
        "            print(\"Best model loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading best_model.pth: {e}\")\n",
        "            print(\"Please ensure that the model architecture matches exactly when saving and loading.\")\n",
        "    else:\n",
        "        print(\"No best model found to load.\")\n",
        "\n",
        "    # Final evaluation\n",
        "    final_acc, final_cm, final_loss = evaluate_model(model, testloader, device)\n",
        "    print(f\"\\nFinal Test Accuracy: {final_acc:.2f}%\")\n",
        "    plot_confusion_matrix(final_cm, classes)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Plot Training and Validation Metrics\n",
        "    # ----------------------------\n",
        "    plot_metrics(train_losses, val_accuracies, val_losses, train_accuracies)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ]
}