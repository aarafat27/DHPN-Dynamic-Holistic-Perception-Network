{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import time"
      ],
      "metadata": {
        "id": "qEHXGdwggOqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Model Architecture Components\n",
        "# ----------------------------\n",
        "\n",
        "class GlobalFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracts global features from the input image.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GlobalFeatureExtractor, self).__init__()\n",
        "        self.global_layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),  # [B, out_channels, H, W]\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # [B, out_channels, H/2, W/2]\n",
        "            nn.Conv2d(out_channels, out_channels * 2, kernel_size=3, padding=1),  # [B, out_channels*2, H/2, W/2]\n",
        "            nn.BatchNorm2d(out_channels * 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1,1))  # [B, out_channels*2, 1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.global_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # [B, out_channels*2]\n",
        "        return x  # Shape: [batch_size, out_channels*2]\n",
        "\n",
        "class LocalFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracts local features from the input image.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(LocalFeatureExtractor, self).__init__()\n",
        "        self.local_layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),  # [B, out_channels, H, W]\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * 2, kernel_size=3, padding=1),  # [B, out_channels*2, H, W]\n",
        "            nn.BatchNorm2d(out_channels * 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)  # [B, out_channels*2, H/2, W/2]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.local_layers(x)\n",
        "        return x  # Shape: [batch_size, out_channels*2, H/2, W/2]\n",
        "\n",
        "class AdaptiveAttentionModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Adaptive Attention Module that weights global and local features.\n",
        "    \"\"\"\n",
        "    def __init__(self, global_dim, local_dim):\n",
        "        super(AdaptiveAttentionModule, self).__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(global_dim + local_dim, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 2),  # Outputs weights for global and local features\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, global_feat, local_feat):\n",
        "        # global_feat: [B, global_dim]\n",
        "        # local_feat: [B, local_dim, H, W]\n",
        "        local_feat_mean = torch.mean(local_feat, dim=[2,3])  # [B, local_dim]\n",
        "        combined = torch.cat((global_feat, local_feat_mean), dim=1)  # [B, global_dim + local_dim]\n",
        "        weights = self.attention(combined)  # [B, 2]\n",
        "        global_weight = weights[:,0].unsqueeze(1)  # [B, 1]\n",
        "        local_weight = weights[:,1].unsqueeze(1)   # [B, 1]\n",
        "        return global_weight, local_weight  # Each [B, 1]\n",
        "\n",
        "class DynamicHolisticPerceptionNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Dynamic Holistic Perception Network combining global and local features.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(DynamicHolisticPerceptionNetwork, self).__init__()\n",
        "        # Initial Convolutional Layers\n",
        "        self.initial_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # [B, 64, 32, 32] for CIFAR-10\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # [B, 64, 16, 16]\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # [B, 64, 16, 16]\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)   # [B, 64, 8, 8]\n",
        "        )\n",
        "\n",
        "        # Feature Extractors\n",
        "        self.global_extractor = GlobalFeatureExtractor(in_channels=64, out_channels=128)  # Output: [B, 256]\n",
        "        self.local_extractor = LocalFeatureExtractor(in_channels=64, out_channels=128)    # Output: [B, 256, 4, 4]\n",
        "\n",
        "        # Adaptive Attention Module\n",
        "        self.adaptive_attention = AdaptiveAttentionModule(global_dim=256, local_dim=256)\n",
        "\n",
        "        # Fusion and Classification Layers\n",
        "        self.fusion_fc = nn.Sequential(\n",
        "            nn.Linear(256 + 256, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial_conv(x)  # [B, 64, 8, 8]\n",
        "        global_feat = self.global_extractor(x)  # [B, 256]\n",
        "        local_feat = self.local_extractor(x)    # [B, 256, 4, 4]\n",
        "        global_weight, local_weight = self.adaptive_attention(global_feat, local_feat)  # Each [B,1]\n",
        "\n",
        "        # Weight the features\n",
        "        global_feat_weighted = global_feat * global_weight  # [B, 256]\n",
        "        local_feat_pooled = F.adaptive_avg_pool2d(local_feat, (1,1)).view(local_feat.size(0), -1)  # [B, 256]\n",
        "        local_feat_weighted = local_feat_pooled * local_weight  # [B, 256]\n",
        "\n",
        "        # Concatenate weighted features\n",
        "        fused_feat = torch.cat((global_feat_weighted, local_feat_weighted), dim=1)  # [B, 512]\n",
        "\n",
        "        # Classification\n",
        "        out = self.fusion_fc(fused_feat)  # [B, num_classes]\n",
        "        return out\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluation Function\n",
        "# ----------------------------\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the given dataloader.\n",
        "    Returns accuracy and confusion matrix.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)  # [B, num_classes]\n",
        "            _, preds = torch.max(outputs, 1)  # [B]\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    acc = accuracy_score(all_labels, all_preds) * 100\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    return acc, cm\n",
        "\n",
        "# ----------------------------\n",
        "# Main Training and Evaluation Pipeline\n",
        "# ----------------------------\n",
        "\n",
        "def main():\n",
        "    # ----------------------------\n",
        "    # Device Configuration\n",
        "    # ----------------------------\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Hyperparameters\n",
        "    # ----------------------------\n",
        "    num_epochs = 20\n",
        "    batch_size = 128\n",
        "    learning_rate = 1e-3\n",
        "    num_classes = 10  # For CIFAR-10\n",
        "\n",
        "    # ----------------------------\n",
        "    # Data Transformations\n",
        "    # ----------------------------\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    # ----------------------------\n",
        "    # Load Datasets\n",
        "    # ----------------------------\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                           download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Initialize Model, Loss, Optimizer\n",
        "    # ----------------------------\n",
        "    model = DynamicHolisticPerceptionNetwork(num_classes=num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Training Loop\n",
        "    # ----------------------------\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)  # [B, num_classes]\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = 100. * correct / total\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        # Validation\n",
        "        val_acc, val_cm = evaluate_model(model, testloader, device)\n",
        "\n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
        "              f\"Loss: {epoch_loss:.4f} - \"\n",
        "              f\"Train Acc: {epoch_acc:.2f}% - \"\n",
        "              f\"Val Acc: {val_acc:.2f}% - \"\n",
        "              f\"Time: {elapsed_time:.2f}s\")\n",
        "\n",
        "        # Save the best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(f\"Best model saved with Val Acc: {best_val_acc:.2f}%\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Final Evaluation\n",
        "    # ----------------------------\n",
        "    print(\"\\nTraining Completed!\")\n",
        "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "    # Load the best model for final evaluation\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    final_acc, final_cm = evaluate_model(model, testloader, device)\n",
        "    print(f\"\\nFinal Test Accuracy: {final_acc:.2f}%\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(final_cm)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toOoRxRLgO2i",
        "outputId": "2a928746-6548-4ec2-c238-ee2518e4cfa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 30.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/20] - Loss: 1.4444 - Train Acc: 46.85% - Val Acc: 58.94% - Time: 30.37s\n",
            "Best model saved with Val Acc: 58.94%\n",
            "Epoch [2/20] - Loss: 1.0599 - Train Acc: 62.18% - Val Acc: 62.29% - Time: 24.17s\n",
            "Best model saved with Val Acc: 62.29%\n",
            "Epoch [3/20] - Loss: 0.8979 - Train Acc: 68.34% - Val Acc: 67.59% - Time: 21.73s\n",
            "Best model saved with Val Acc: 67.59%\n",
            "Epoch [4/20] - Loss: 0.7947 - Train Acc: 72.15% - Val Acc: 67.43% - Time: 21.18s\n",
            "Epoch [5/20] - Loss: 0.7330 - Train Acc: 74.32% - Val Acc: 70.59% - Time: 23.07s\n",
            "Best model saved with Val Acc: 70.59%\n",
            "Epoch [6/20] - Loss: 0.6797 - Train Acc: 76.48% - Val Acc: 74.75% - Time: 21.17s\n",
            "Best model saved with Val Acc: 74.75%\n",
            "Epoch [7/20] - Loss: 0.6388 - Train Acc: 77.88% - Val Acc: 74.43% - Time: 21.11s\n",
            "Epoch [8/20] - Loss: 0.6095 - Train Acc: 79.06% - Val Acc: 75.38% - Time: 22.56s\n",
            "Best model saved with Val Acc: 75.38%\n",
            "Epoch [9/20] - Loss: 0.5775 - Train Acc: 80.14% - Val Acc: 77.14% - Time: 20.84s\n",
            "Best model saved with Val Acc: 77.14%\n",
            "Epoch [10/20] - Loss: 0.5521 - Train Acc: 80.79% - Val Acc: 81.18% - Time: 19.92s\n",
            "Best model saved with Val Acc: 81.18%\n",
            "Epoch [11/20] - Loss: 0.4563 - Train Acc: 84.35% - Val Acc: 83.24% - Time: 22.07s\n",
            "Best model saved with Val Acc: 83.24%\n",
            "Epoch [12/20] - Loss: 0.4318 - Train Acc: 84.99% - Val Acc: 83.64% - Time: 20.27s\n",
            "Best model saved with Val Acc: 83.64%\n",
            "Epoch [13/20] - Loss: 0.4199 - Train Acc: 85.54% - Val Acc: 83.91% - Time: 20.20s\n",
            "Best model saved with Val Acc: 83.91%\n",
            "Epoch [14/20] - Loss: 0.4133 - Train Acc: 85.80% - Val Acc: 84.26% - Time: 21.99s\n",
            "Best model saved with Val Acc: 84.26%\n",
            "Epoch [15/20] - Loss: 0.4060 - Train Acc: 85.93% - Val Acc: 84.17% - Time: 19.96s\n",
            "Epoch [16/20] - Loss: 0.3940 - Train Acc: 86.38% - Val Acc: 84.27% - Time: 20.30s\n",
            "Best model saved with Val Acc: 84.27%\n",
            "Epoch [17/20] - Loss: 0.3904 - Train Acc: 86.35% - Val Acc: 84.63% - Time: 22.13s\n",
            "Best model saved with Val Acc: 84.63%\n",
            "Epoch [18/20] - Loss: 0.3881 - Train Acc: 86.66% - Val Acc: 84.49% - Time: 20.47s\n",
            "Epoch [19/20] - Loss: 0.3835 - Train Acc: 86.78% - Val Acc: 84.60% - Time: 31.19s\n",
            "Epoch [20/20] - Loss: 0.3782 - Train Acc: 86.79% - Val Acc: 84.69% - Time: 20.64s\n",
            "Best model saved with Val Acc: 84.69%\n",
            "\n",
            "Training Completed!\n",
            "Best Validation Accuracy: 84.69%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-15d73d287cb0>:262: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Accuracy: 84.69%\n",
            "Confusion Matrix:\n",
            "[[872  14  27  13  13   1   5   9  35  11]\n",
            " [  6 949   1   2   1   0   1   3   8  29]\n",
            " [ 33   1 807  25  58  23  31  13   5   4]\n",
            " [ 14   2  64 650  61 122  37  29  15   6]\n",
            " [  9   2  42  21 868  13  20  22   2   1]\n",
            " [  9   1  40 122  38 739  10  38   1   2]\n",
            " [  6   1  49  28  13   9 889   2   2   1]\n",
            " [ 10   2  18  21  38  34   3 871   1   2]\n",
            " [ 33  10   2   6   3   0   3   4 931   8]\n",
            " [ 18  58   5   4   0   0   4   2  16 893]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6xSog2LUiUOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Model Architecture Components\n",
        "# ----------------------------\n",
        "\n",
        "class GlobalFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracts global features from the input image.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GlobalFeatureExtractor, self).__init__()\n",
        "        self.global_layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),  # [B, out_channels, H, W]\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # [B, out_channels, H/2, W/2]\n",
        "            nn.Conv2d(out_channels, out_channels * 2, kernel_size=3, padding=1),  # [B, out_channels*2, H/2, W/2]\n",
        "            nn.BatchNorm2d(out_channels * 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1,1))  # [B, out_channels*2, 1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.global_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # [B, out_channels*2]\n",
        "        return x  # Shape: [batch_size, out_channels*2]\n",
        "\n",
        "class LocalFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracts local features from the input image.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(LocalFeatureExtractor, self).__init__()\n",
        "        self.local_layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),  # [B, out_channels, H, W]\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * 2, kernel_size=3, padding=1),  # [B, out_channels*2, H, W]\n",
        "            nn.BatchNorm2d(out_channels * 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)  # [B, out_channels*2, H/2, W/2]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.local_layers(x)\n",
        "        return x  # Shape: [batch_size, out_channels*2, H/2, W/2]\n",
        "\n",
        "class AdaptiveAttentionModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Adaptive Attention Module that weights global and local features.\n",
        "    \"\"\"\n",
        "    def __init__(self, global_dim, local_dim):\n",
        "        super(AdaptiveAttentionModule, self).__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(global_dim + local_dim, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 2),  # Outputs weights for global and local features\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, global_feat, local_feat):\n",
        "        # global_feat: [B, global_dim]\n",
        "        # local_feat: [B, local_dim, H, W]\n",
        "        local_feat_mean = torch.mean(local_feat, dim=[2,3])  # [B, local_dim]\n",
        "        combined = torch.cat((global_feat, local_feat_mean), dim=1)  # [B, global_dim + local_dim]\n",
        "        weights = self.attention(combined)  # [B, 2]\n",
        "        global_weight = weights[:,0].unsqueeze(1)  # [B, 1]\n",
        "        local_weight = weights[:,1].unsqueeze(1)   # [B, 1]\n",
        "        return global_weight, local_weight  # Each [B, 1]\n",
        "\n",
        "class DynamicHolisticPerceptionNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Dynamic Holistic Perception Network combining global and local features.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(DynamicHolisticPerceptionNetwork, self).__init__()\n",
        "        # Initial Convolutional Layers\n",
        "        self.initial_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # [B, 64, 32, 32] for CIFAR-10\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # [B, 64, 16, 16]\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # [B, 64, 16, 16]\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)   # [B, 64, 8, 8]\n",
        "        )\n",
        "\n",
        "        # Feature Extractors\n",
        "        self.global_extractor = GlobalFeatureExtractor(in_channels=64, out_channels=128)  # Output: [B, 256]\n",
        "        self.local_extractor = LocalFeatureExtractor(in_channels=64, out_channels=128)    # Output: [B, 256, 4, 4]\n",
        "\n",
        "        # Adaptive Attention Module\n",
        "        self.adaptive_attention = AdaptiveAttentionModule(global_dim=256, local_dim=256)\n",
        "\n",
        "        # Fusion and Classification Layers\n",
        "        self.fusion_fc = nn.Sequential(\n",
        "            nn.Linear(256 + 256, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial_conv(x)  # [B, 64, 8, 8]\n",
        "        global_feat = self.global_extractor(x)  # [B, 256]\n",
        "        local_feat = self.local_extractor(x)    # [B, 256, 4, 4]\n",
        "        global_weight, local_weight = self.adaptive_attention(global_feat, local_feat)  # Each [B,1]\n",
        "\n",
        "        # Weight the features\n",
        "        global_feat_weighted = global_feat * global_weight  # [B, 256]\n",
        "        local_feat_pooled = F.adaptive_avg_pool2d(local_feat, (1,1)).view(local_feat.size(0), -1)  # [B, 256]\n",
        "        local_feat_weighted = local_feat_pooled * local_weight  # [B, 256]\n",
        "\n",
        "        # Concatenate weighted features\n",
        "        fused_feat = torch.cat((global_feat_weighted, local_feat_weighted), dim=1)  # [B, 512]\n",
        "\n",
        "        # Classification\n",
        "        out = self.fusion_fc(fused_feat)  # [B, num_classes]\n",
        "        return out\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluation Function\n",
        "# ----------------------------\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the given dataloader.\n",
        "    Returns accuracy and confusion matrix.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)  # [B, num_classes]\n",
        "            _, preds = torch.max(outputs, 1)  # [B]\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    acc = accuracy_score(all_labels, all_preds) * 100\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    return acc, cm\n",
        "\n",
        "# ----------------------------\n",
        "# Main Training and Evaluation Pipeline\n",
        "# ----------------------------\n",
        "\n",
        "def main():\n",
        "    # ----------------------------\n",
        "    # Device Configuration\n",
        "    # ----------------------------\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Hyperparameters\n",
        "    # ----------------------------\n",
        "    num_epochs = 100\n",
        "    batch_size = 128\n",
        "    learning_rate = 1e-3\n",
        "    num_classes = 10  # For CIFAR-10\n",
        "\n",
        "    # ----------------------------\n",
        "    # Data Transformations\n",
        "    # ----------------------------\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    # ----------------------------\n",
        "    # Load Datasets\n",
        "    # ----------------------------\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                           download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Initialize Model, Loss, Optimizer\n",
        "    # ----------------------------\n",
        "    model = DynamicHolisticPerceptionNetwork(num_classes=num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Training Loop\n",
        "    # ----------------------------\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)  # [B, num_classes]\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = 100. * correct / total\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        # Validation\n",
        "        val_acc, val_cm = evaluate_model(model, testloader, device)\n",
        "\n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
        "              f\"Loss: {epoch_loss:.4f} - \"\n",
        "              f\"Train Acc: {epoch_acc:.2f}% - \"\n",
        "              f\"Val Acc: {val_acc:.2f}% - \"\n",
        "              f\"Time: {elapsed_time:.2f}s\")\n",
        "\n",
        "        # Save the best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(f\"Best model saved with Val Acc: {best_val_acc:.2f}%\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Final Evaluation\n",
        "    # ----------------------------\n",
        "    print(\"\\nTraining Completed!\")\n",
        "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "    # Load the best model for final evaluation\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    final_acc, final_cm = evaluate_model(model, testloader, device)\n",
        "    print(f\"\\nFinal Test Accuracy: {final_acc:.2f}%\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(final_cm)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMFNZTLAiUgS",
        "outputId": "845e5443-a0d3-4dd3-a843-78e5cc9e3d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/100] - Loss: 1.4398 - Train Acc: 46.47% - Val Acc: 54.80% - Time: 20.36s\n",
            "Best model saved with Val Acc: 54.80%\n",
            "Epoch [2/100] - Loss: 1.0561 - Train Acc: 62.32% - Val Acc: 56.22% - Time: 20.11s\n",
            "Best model saved with Val Acc: 56.22%\n",
            "Epoch [3/100] - Loss: 0.9075 - Train Acc: 67.89% - Val Acc: 68.89% - Time: 22.26s\n",
            "Best model saved with Val Acc: 68.89%\n",
            "Epoch [4/100] - Loss: 0.8072 - Train Acc: 71.58% - Val Acc: 70.52% - Time: 20.13s\n",
            "Best model saved with Val Acc: 70.52%\n",
            "Epoch [5/100] - Loss: 0.7428 - Train Acc: 74.08% - Val Acc: 69.11% - Time: 20.27s\n",
            "Epoch [6/100] - Loss: 0.6951 - Train Acc: 75.80% - Val Acc: 74.06% - Time: 21.99s\n",
            "Best model saved with Val Acc: 74.06%\n",
            "Epoch [7/100] - Loss: 0.6561 - Train Acc: 77.22% - Val Acc: 75.05% - Time: 20.21s\n",
            "Best model saved with Val Acc: 75.05%\n",
            "Epoch [8/100] - Loss: 0.6250 - Train Acc: 78.52% - Val Acc: 78.21% - Time: 21.34s\n",
            "Best model saved with Val Acc: 78.21%\n",
            "Epoch [9/100] - Loss: 0.5941 - Train Acc: 79.54% - Val Acc: 76.64% - Time: 21.05s\n",
            "Epoch [10/100] - Loss: 0.5726 - Train Acc: 80.19% - Val Acc: 76.06% - Time: 20.28s\n",
            "Epoch [11/100] - Loss: 0.4861 - Train Acc: 83.09% - Val Acc: 82.07% - Time: 21.92s\n",
            "Best model saved with Val Acc: 82.07%\n",
            "Epoch [12/100] - Loss: 0.4596 - Train Acc: 84.04% - Val Acc: 82.37% - Time: 20.76s\n",
            "Best model saved with Val Acc: 82.37%\n",
            "Epoch [13/100] - Loss: 0.4487 - Train Acc: 84.46% - Val Acc: 82.97% - Time: 20.19s\n",
            "Best model saved with Val Acc: 82.97%\n",
            "Epoch [14/100] - Loss: 0.4380 - Train Acc: 84.71% - Val Acc: 82.82% - Time: 21.98s\n",
            "Epoch [15/100] - Loss: 0.4339 - Train Acc: 84.97% - Val Acc: 83.28% - Time: 21.15s\n",
            "Best model saved with Val Acc: 83.28%\n",
            "Epoch [16/100] - Loss: 0.4306 - Train Acc: 85.13% - Val Acc: 83.15% - Time: 20.15s\n",
            "Epoch [17/100] - Loss: 0.4218 - Train Acc: 85.47% - Val Acc: 83.38% - Time: 22.01s\n",
            "Best model saved with Val Acc: 83.38%\n",
            "Epoch [18/100] - Loss: 0.4213 - Train Acc: 85.53% - Val Acc: 83.48% - Time: 20.57s\n",
            "Best model saved with Val Acc: 83.48%\n",
            "Epoch [19/100] - Loss: 0.4111 - Train Acc: 85.70% - Val Acc: 83.42% - Time: 20.92s\n",
            "Epoch [20/100] - Loss: 0.4083 - Train Acc: 85.84% - Val Acc: 83.41% - Time: 23.22s\n",
            "Epoch [21/100] - Loss: 0.3950 - Train Acc: 86.32% - Val Acc: 83.59% - Time: 21.77s\n",
            "Best model saved with Val Acc: 83.59%\n",
            "Epoch [22/100] - Loss: 0.3928 - Train Acc: 86.32% - Val Acc: 83.65% - Time: 20.90s\n",
            "Best model saved with Val Acc: 83.65%\n",
            "Epoch [23/100] - Loss: 0.3924 - Train Acc: 86.46% - Val Acc: 83.74% - Time: 22.78s\n",
            "Best model saved with Val Acc: 83.74%\n",
            "Epoch [24/100] - Loss: 0.3923 - Train Acc: 86.43% - Val Acc: 83.83% - Time: 22.35s\n",
            "Best model saved with Val Acc: 83.83%\n",
            "Epoch [25/100] - Loss: 0.3917 - Train Acc: 86.38% - Val Acc: 83.82% - Time: 20.71s\n",
            "Epoch [26/100] - Loss: 0.3912 - Train Acc: 86.44% - Val Acc: 83.89% - Time: 22.09s\n",
            "Best model saved with Val Acc: 83.89%\n",
            "Epoch [27/100] - Loss: 0.3888 - Train Acc: 86.50% - Val Acc: 83.88% - Time: 22.66s\n",
            "Epoch [28/100] - Loss: 0.3873 - Train Acc: 86.63% - Val Acc: 83.85% - Time: 21.56s\n",
            "Epoch [29/100] - Loss: 0.3897 - Train Acc: 86.70% - Val Acc: 84.13% - Time: 21.62s\n",
            "Best model saved with Val Acc: 84.13%\n",
            "Epoch [30/100] - Loss: 0.3884 - Train Acc: 86.30% - Val Acc: 83.86% - Time: 23.06s\n",
            "Epoch [31/100] - Loss: 0.3827 - Train Acc: 86.82% - Val Acc: 83.92% - Time: 21.62s\n",
            "Epoch [32/100] - Loss: 0.3851 - Train Acc: 86.61% - Val Acc: 83.81% - Time: 21.02s\n",
            "Epoch [33/100] - Loss: 0.3896 - Train Acc: 86.52% - Val Acc: 84.00% - Time: 22.22s\n",
            "Epoch [34/100] - Loss: 0.3833 - Train Acc: 86.80% - Val Acc: 83.85% - Time: 21.16s\n",
            "Epoch [35/100] - Loss: 0.3894 - Train Acc: 86.52% - Val Acc: 83.91% - Time: 20.50s\n",
            "Epoch [36/100] - Loss: 0.3870 - Train Acc: 86.56% - Val Acc: 83.72% - Time: 22.18s\n",
            "Epoch [37/100] - Loss: 0.3864 - Train Acc: 86.59% - Val Acc: 84.11% - Time: 20.32s\n",
            "Epoch [38/100] - Loss: 0.3835 - Train Acc: 86.73% - Val Acc: 83.88% - Time: 20.15s\n",
            "Epoch [39/100] - Loss: 0.3879 - Train Acc: 86.31% - Val Acc: 84.08% - Time: 22.13s\n",
            "Epoch [40/100] - Loss: 0.3863 - Train Acc: 86.71% - Val Acc: 83.84% - Time: 20.44s\n",
            "Epoch [41/100] - Loss: 0.3882 - Train Acc: 86.51% - Val Acc: 83.97% - Time: 20.24s\n",
            "Epoch [42/100] - Loss: 0.3847 - Train Acc: 86.64% - Val Acc: 84.04% - Time: 22.12s\n",
            "Epoch [43/100] - Loss: 0.3842 - Train Acc: 86.67% - Val Acc: 83.85% - Time: 20.48s\n",
            "Epoch [44/100] - Loss: 0.3835 - Train Acc: 86.64% - Val Acc: 83.88% - Time: 21.02s\n",
            "Epoch [45/100] - Loss: 0.3844 - Train Acc: 86.66% - Val Acc: 84.09% - Time: 22.87s\n",
            "Epoch [46/100] - Loss: 0.3863 - Train Acc: 86.66% - Val Acc: 83.83% - Time: 20.35s\n",
            "Epoch [47/100] - Loss: 0.3848 - Train Acc: 86.70% - Val Acc: 83.95% - Time: 20.58s\n",
            "Epoch [48/100] - Loss: 0.3878 - Train Acc: 86.54% - Val Acc: 84.05% - Time: 22.17s\n",
            "Epoch [49/100] - Loss: 0.3874 - Train Acc: 86.62% - Val Acc: 84.09% - Time: 20.43s\n",
            "Epoch [50/100] - Loss: 0.3834 - Train Acc: 86.69% - Val Acc: 83.95% - Time: 20.55s\n",
            "Epoch [51/100] - Loss: 0.3880 - Train Acc: 86.53% - Val Acc: 84.09% - Time: 22.12s\n",
            "Epoch [52/100] - Loss: 0.3862 - Train Acc: 86.69% - Val Acc: 83.88% - Time: 20.33s\n",
            "Epoch [53/100] - Loss: 0.3851 - Train Acc: 86.74% - Val Acc: 83.94% - Time: 20.34s\n",
            "Epoch [54/100] - Loss: 0.3874 - Train Acc: 86.54% - Val Acc: 83.91% - Time: 22.59s\n",
            "Epoch [55/100] - Loss: 0.3881 - Train Acc: 86.41% - Val Acc: 83.99% - Time: 20.60s\n",
            "Epoch [56/100] - Loss: 0.3875 - Train Acc: 86.56% - Val Acc: 84.13% - Time: 20.57s\n",
            "Epoch [57/100] - Loss: 0.3854 - Train Acc: 86.66% - Val Acc: 83.94% - Time: 22.08s\n",
            "Epoch [58/100] - Loss: 0.3872 - Train Acc: 86.55% - Val Acc: 83.88% - Time: 20.46s\n",
            "Epoch [59/100] - Loss: 0.3865 - Train Acc: 86.56% - Val Acc: 83.89% - Time: 21.62s\n",
            "Epoch [60/100] - Loss: 0.3851 - Train Acc: 86.54% - Val Acc: 83.88% - Time: 21.40s\n",
            "Epoch [61/100] - Loss: 0.3877 - Train Acc: 86.73% - Val Acc: 83.76% - Time: 20.66s\n",
            "Epoch [62/100] - Loss: 0.3825 - Train Acc: 86.67% - Val Acc: 83.91% - Time: 21.70s\n",
            "Epoch [63/100] - Loss: 0.3859 - Train Acc: 86.48% - Val Acc: 83.99% - Time: 22.08s\n",
            "Epoch [64/100] - Loss: 0.3840 - Train Acc: 86.69% - Val Acc: 83.96% - Time: 20.64s\n",
            "Epoch [65/100] - Loss: 0.3874 - Train Acc: 86.60% - Val Acc: 83.93% - Time: 21.60s\n",
            "Epoch [66/100] - Loss: 0.3818 - Train Acc: 86.74% - Val Acc: 83.97% - Time: 21.72s\n",
            "Epoch [67/100] - Loss: 0.3838 - Train Acc: 86.78% - Val Acc: 83.97% - Time: 20.59s\n",
            "Epoch [68/100] - Loss: 0.3878 - Train Acc: 86.71% - Val Acc: 84.04% - Time: 22.23s\n",
            "Epoch [69/100] - Loss: 0.3845 - Train Acc: 86.59% - Val Acc: 83.94% - Time: 21.50s\n",
            "Epoch [70/100] - Loss: 0.3854 - Train Acc: 86.54% - Val Acc: 83.95% - Time: 20.38s\n",
            "Epoch [71/100] - Loss: 0.3858 - Train Acc: 86.58% - Val Acc: 84.00% - Time: 22.18s\n",
            "Epoch [72/100] - Loss: 0.3854 - Train Acc: 86.66% - Val Acc: 84.06% - Time: 20.92s\n",
            "Epoch [73/100] - Loss: 0.3891 - Train Acc: 86.59% - Val Acc: 84.17% - Time: 20.42s\n",
            "Best model saved with Val Acc: 84.17%\n",
            "Epoch [74/100] - Loss: 0.3875 - Train Acc: 86.54% - Val Acc: 83.95% - Time: 22.11s\n",
            "Epoch [75/100] - Loss: 0.3891 - Train Acc: 86.57% - Val Acc: 83.95% - Time: 20.09s\n",
            "Epoch [76/100] - Loss: 0.3844 - Train Acc: 86.46% - Val Acc: 83.98% - Time: 20.63s\n",
            "Epoch [77/100] - Loss: 0.3835 - Train Acc: 86.70% - Val Acc: 84.15% - Time: 21.95s\n",
            "Epoch [78/100] - Loss: 0.3880 - Train Acc: 86.44% - Val Acc: 84.05% - Time: 20.29s\n",
            "Epoch [79/100] - Loss: 0.3850 - Train Acc: 86.57% - Val Acc: 83.96% - Time: 20.51s\n",
            "Epoch [80/100] - Loss: 0.3890 - Train Acc: 86.60% - Val Acc: 84.05% - Time: 22.42s\n",
            "Epoch [81/100] - Loss: 0.3847 - Train Acc: 86.68% - Val Acc: 84.12% - Time: 20.57s\n",
            "Epoch [82/100] - Loss: 0.3849 - Train Acc: 86.63% - Val Acc: 83.94% - Time: 20.72s\n",
            "Epoch [83/100] - Loss: 0.3867 - Train Acc: 86.66% - Val Acc: 84.04% - Time: 22.36s\n",
            "Epoch [84/100] - Loss: 0.3820 - Train Acc: 86.67% - Val Acc: 83.82% - Time: 20.43s\n",
            "Epoch [85/100] - Loss: 0.3863 - Train Acc: 86.65% - Val Acc: 83.77% - Time: 20.66s\n",
            "Epoch [86/100] - Loss: 0.3875 - Train Acc: 86.56% - Val Acc: 83.89% - Time: 22.34s\n",
            "Epoch [87/100] - Loss: 0.3864 - Train Acc: 86.45% - Val Acc: 84.09% - Time: 20.67s\n",
            "Epoch [88/100] - Loss: 0.3888 - Train Acc: 86.44% - Val Acc: 83.92% - Time: 21.64s\n",
            "Epoch [89/100] - Loss: 0.3865 - Train Acc: 86.59% - Val Acc: 83.77% - Time: 22.33s\n",
            "Epoch [90/100] - Loss: 0.3851 - Train Acc: 86.71% - Val Acc: 84.14% - Time: 20.43s\n",
            "Epoch [91/100] - Loss: 0.3840 - Train Acc: 86.58% - Val Acc: 83.88% - Time: 22.36s\n",
            "Epoch [92/100] - Loss: 0.3863 - Train Acc: 86.63% - Val Acc: 83.94% - Time: 22.25s\n",
            "Epoch [93/100] - Loss: 0.3898 - Train Acc: 86.41% - Val Acc: 84.09% - Time: 20.63s\n",
            "Epoch [94/100] - Loss: 0.3839 - Train Acc: 86.71% - Val Acc: 83.97% - Time: 21.86s\n",
            "Epoch [95/100] - Loss: 0.3837 - Train Acc: 86.82% - Val Acc: 83.90% - Time: 21.91s\n",
            "Epoch [96/100] - Loss: 0.3868 - Train Acc: 86.74% - Val Acc: 84.00% - Time: 20.57s\n",
            "Epoch [97/100] - Loss: 0.3862 - Train Acc: 86.55% - Val Acc: 84.16% - Time: 22.01s\n",
            "Epoch [98/100] - Loss: 0.3861 - Train Acc: 86.60% - Val Acc: 84.24% - Time: 21.82s\n",
            "Best model saved with Val Acc: 84.24%\n",
            "Epoch [99/100] - Loss: 0.3874 - Train Acc: 86.55% - Val Acc: 83.87% - Time: 20.44s\n",
            "Epoch [100/100] - Loss: 0.3851 - Train Acc: 86.66% - Val Acc: 84.02% - Time: 22.32s\n",
            "\n",
            "Training Completed!\n",
            "Best Validation Accuracy: 84.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-f04e0458004e>:262: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Accuracy: 84.24%\n",
            "Confusion Matrix:\n",
            "[[866  10  25  10  13   0   5  11  37  23]\n",
            " [  7 932   0   4   0   1   2   1  11  42]\n",
            " [ 39   3 782  43  48  29  36  12   4   4]\n",
            " [ 16   3  41 680  56 117  46  24   8   9]\n",
            " [  6   2  34  27 848  22  30  26   3   2]\n",
            " [ 11   2  23 125  36 754  11  34   0   4]\n",
            " [  6   2  30  40  17  12 882   4   4   3]\n",
            " [ 10   1  17  30  29  38   3 864   1   7]\n",
            " [ 45  15   1   4   1   2   5   2 914  11]\n",
            " [ 17  48   3   8   1   0   2   5  14 902]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With ResidualBlock"
      ],
      "metadata": {
        "id": "6cPh8k3ZHnFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import time\n",
        "\n",
        "# ----------------------------\n",
        "# Model Architecture Components with Residual Connections\n",
        "# ----------------------------\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard residual block with two convolutional layers and a skip connection.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
        "                               kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
        "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Shortcut connection to match dimensions\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class GlobalFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracts global features from the input image with residual connections.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GlobalFeatureExtractor, self).__init__()\n",
        "        self.layer1 = ResidualBlock(in_channels, out_channels, stride=1)\n",
        "        self.layer2 = ResidualBlock(out_channels, out_channels * 2, stride=2)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)  # [B, out_channels, H, W]\n",
        "        x = self.layer2(x)  # [B, out_channels*2, H/2, W/2]\n",
        "        x = self.pool(x)     # [B, out_channels*2, 1, 1]\n",
        "        x = x.view(x.size(0), -1)  # [B, out_channels*2]\n",
        "        return x\n",
        "\n",
        "class LocalFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracts local features from the input image with residual connections.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(LocalFeatureExtractor, self).__init__()\n",
        "        self.layer1 = ResidualBlock(in_channels, out_channels, stride=1)\n",
        "        self.layer2 = ResidualBlock(out_channels, out_channels * 2, stride=2)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((2,2))  # Retains some spatial information\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)  # [B, out_channels, H, W]\n",
        "        x = self.layer2(x)  # [B, out_channels*2, H/2, W/2]\n",
        "        x = self.pool(x)     # [B, out_channels*2, 2, 2]\n",
        "        return x\n",
        "\n",
        "class AdaptiveAttentionModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Adaptive Attention Module that weights global and local features.\n",
        "    \"\"\"\n",
        "    def __init__(self, global_dim, local_dim):\n",
        "        super(AdaptiveAttentionModule, self).__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(global_dim + local_dim, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 2),  # Outputs weights for global and local features\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, global_feat, local_feat):\n",
        "        # global_feat: [B, global_dim]\n",
        "        # local_feat: [B, local_dim, H, W]\n",
        "        local_feat_mean = torch.mean(local_feat, dim=[2,3])  # [B, local_dim]\n",
        "        combined = torch.cat((global_feat, local_feat_mean), dim=1)  # [B, global_dim + local_dim]\n",
        "        weights = self.attention(combined)  # [B, 2]\n",
        "        global_weight = weights[:,0].unsqueeze(1)  # [B, 1]\n",
        "        local_weight = weights[:,1].unsqueeze(1)   # [B, 1]\n",
        "        return global_weight, local_weight  # Each [B, 1]\n",
        "\n",
        "class DynamicHolisticPerceptionNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Dynamic Holistic Perception Network combining global and local features with residual connections.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(DynamicHolisticPerceptionNetwork, self).__init__()\n",
        "        # Initial Convolutional Layers\n",
        "        self.initial_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # [B, 64, 32, 32] for CIFAR-10\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),  # [B, 64, 16, 16]\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # [B, 64, 16, 16]\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)   # [B, 64, 8, 8]\n",
        "        )\n",
        "\n",
        "        # Feature Extractors with Residual Blocks\n",
        "        self.global_extractor = GlobalFeatureExtractor(in_channels=64, out_channels=128)  # Output: [B, 256]\n",
        "        self.local_extractor = LocalFeatureExtractor(in_channels=64, out_channels=128)    # Output: [B, 256, 2, 2]\n",
        "\n",
        "        # Adaptive Attention Module\n",
        "        self.adaptive_attention = AdaptiveAttentionModule(global_dim=256, local_dim=256)\n",
        "\n",
        "        # Fusion and Classification Layers\n",
        "        self.fusion_fc = nn.Sequential(\n",
        "            nn.Linear(256 + 256, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial_conv(x)  # [B, 64, 8, 8]\n",
        "        global_feat = self.global_extractor(x)  # [B, 256]\n",
        "        local_feat = self.local_extractor(x)    # [B, 256, 2, 2]\n",
        "        global_weight, local_weight = self.adaptive_attention(global_feat, local_feat)  # Each [B,1]\n",
        "\n",
        "        # Weight the features\n",
        "        global_feat_weighted = global_feat * global_weight  # [B, 256]\n",
        "        local_feat_pooled = F.adaptive_avg_pool2d(local_feat, (1,1)).view(local_feat.size(0), -1)  # [B, 256]\n",
        "        local_feat_weighted = local_feat_pooled * local_weight  # [B, 256]\n",
        "\n",
        "        # Concatenate weighted features\n",
        "        fused_feat = torch.cat((global_feat_weighted, local_feat_weighted), dim=1)  # [B, 512]\n",
        "\n",
        "        # Classification\n",
        "        out = self.fusion_fc(fused_feat)  # [B, num_classes]\n",
        "        return out\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluation Function\n",
        "# ----------------------------\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the given dataloader.\n",
        "    Returns accuracy and confusion matrix.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)  # [B, num_classes]\n",
        "            _, preds = torch.max(outputs, 1)  # [B]\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    acc = accuracy_score(all_labels, all_preds) * 100\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    return acc, cm\n",
        "\n",
        "# ----------------------------\n",
        "# Main Training and Evaluation Pipeline\n",
        "# ----------------------------\n",
        "\n",
        "def main():\n",
        "    # ----------------------------\n",
        "    # Device Configuration\n",
        "    # ----------------------------\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Hyperparameters\n",
        "    # ----------------------------\n",
        "    num_epochs = 50  # Increased epochs for better convergence\n",
        "    batch_size = 128\n",
        "    learning_rate = 1e-3\n",
        "    num_classes = 10  # For CIFAR-10\n",
        "\n",
        "    # ----------------------------\n",
        "    # Data Transformations\n",
        "    # ----------------------------\n",
        "    # Removed all data augmentation transforms as per user request\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    # ----------------------------\n",
        "    # Load Datasets\n",
        "    # ----------------------------\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                           download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Initialize Model, Loss, Optimizer\n",
        "    # ----------------------------\n",
        "    model = DynamicHolisticPerceptionNetwork(num_classes=num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)  # Added weight decay for regularization\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # Adjusted scheduler step size\n",
        "\n",
        "    # ----------------------------\n",
        "    # Training Loop\n",
        "    # ----------------------------\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)  # [B, num_classes]\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = 100. * correct / total\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        # Validation\n",
        "        val_acc, val_cm = evaluate_model(model, testloader, device)\n",
        "\n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
        "              f\"Loss: {epoch_loss:.4f} - \"\n",
        "              f\"Train Acc: {epoch_acc:.2f}% - \"\n",
        "              f\"Val Acc: {val_acc:.2f}% - \"\n",
        "              f\"Time: {elapsed_time:.2f}s\")\n",
        "\n",
        "        # Save the best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(f\"Best model saved with Val Acc: {best_val_acc:.2f}%\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Final Evaluation\n",
        "    # ----------------------------\n",
        "    print(\"\\nTraining Completed!\")\n",
        "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "    # Load the best model for final evaluation with security warning addressed\n",
        "    torch_version = torch.__version__\n",
        "    print(f\"PyTorch Version: {torch_version}\")\n",
        "\n",
        "    # Attempt to use weights_only=True if supported\n",
        "    try:\n",
        "        state_dict = torch.load('best_model.pth', weights_only=True)\n",
        "    except TypeError:\n",
        "        # weights_only not supported; fallback to default loading\n",
        "        state_dict = torch.load('best_model.pth')\n",
        "        print(\"weights_only parameter not supported in this PyTorch version. \"\n",
        "              \"Consider upgrading PyTorch for enhanced security.\")\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    final_acc, final_cm = evaluate_model(model, testloader, device)\n",
        "    print(f\"\\nFinal Test Accuracy: {final_acc:.2f}%\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(final_cm)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS-KbPN5HnQ1",
        "outputId": "2103bc2e-4da7-4180-a30d-3f755e16b4fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/50] - Loss: 1.3251 - Train Acc: 51.45% - Val Acc: 62.81% - Time: 25.09s\n",
            "Best model saved with Val Acc: 62.81%\n",
            "Epoch [2/50] - Loss: 0.9114 - Train Acc: 67.81% - Val Acc: 69.06% - Time: 16.00s\n",
            "Best model saved with Val Acc: 69.06%\n",
            "Epoch [3/50] - Loss: 0.7323 - Train Acc: 74.54% - Val Acc: 73.65% - Time: 16.32s\n",
            "Best model saved with Val Acc: 73.65%\n",
            "Epoch [4/50] - Loss: 0.6154 - Train Acc: 78.69% - Val Acc: 75.33% - Time: 16.12s\n",
            "Best model saved with Val Acc: 75.33%\n",
            "Epoch [5/50] - Loss: 0.5231 - Train Acc: 81.93% - Val Acc: 76.34% - Time: 17.66s\n",
            "Best model saved with Val Acc: 76.34%\n",
            "Epoch [6/50] - Loss: 0.4506 - Train Acc: 84.45% - Val Acc: 77.25% - Time: 16.13s\n",
            "Best model saved with Val Acc: 77.25%\n",
            "Epoch [7/50] - Loss: 0.3815 - Train Acc: 86.83% - Val Acc: 78.21% - Time: 16.23s\n",
            "Best model saved with Val Acc: 78.21%\n",
            "Epoch [8/50] - Loss: 0.3224 - Train Acc: 88.93% - Val Acc: 78.40% - Time: 16.12s\n",
            "Best model saved with Val Acc: 78.40%\n",
            "Epoch [9/50] - Loss: 0.2758 - Train Acc: 90.51% - Val Acc: 79.76% - Time: 16.47s\n",
            "Best model saved with Val Acc: 79.76%\n",
            "Epoch [10/50] - Loss: 0.2279 - Train Acc: 92.27% - Val Acc: 77.12% - Time: 16.44s\n",
            "Epoch [11/50] - Loss: 0.1925 - Train Acc: 93.35% - Val Acc: 75.70% - Time: 16.24s\n",
            "Epoch [12/50] - Loss: 0.1681 - Train Acc: 94.31% - Val Acc: 80.24% - Time: 16.42s\n",
            "Best model saved with Val Acc: 80.24%\n",
            "Epoch [13/50] - Loss: 0.1435 - Train Acc: 95.16% - Val Acc: 79.13% - Time: 16.40s\n",
            "Epoch [14/50] - Loss: 0.1293 - Train Acc: 95.57% - Val Acc: 79.67% - Time: 17.14s\n",
            "Epoch [15/50] - Loss: 0.1139 - Train Acc: 96.13% - Val Acc: 80.63% - Time: 16.08s\n",
            "Best model saved with Val Acc: 80.63%\n",
            "Epoch [16/50] - Loss: 0.1110 - Train Acc: 96.24% - Val Acc: 80.33% - Time: 16.31s\n",
            "Epoch [17/50] - Loss: 0.0993 - Train Acc: 96.66% - Val Acc: 80.25% - Time: 16.57s\n",
            "Epoch [18/50] - Loss: 0.0935 - Train Acc: 96.87% - Val Acc: 80.91% - Time: 16.23s\n",
            "Best model saved with Val Acc: 80.91%\n",
            "Epoch [19/50] - Loss: 0.0949 - Train Acc: 96.87% - Val Acc: 80.62% - Time: 16.25s\n",
            "Epoch [20/50] - Loss: 0.0846 - Train Acc: 97.18% - Val Acc: 80.49% - Time: 16.43s\n",
            "Epoch [21/50] - Loss: 0.0281 - Train Acc: 99.16% - Val Acc: 83.32% - Time: 16.39s\n",
            "Best model saved with Val Acc: 83.32%\n",
            "Epoch [22/50] - Loss: 0.0089 - Train Acc: 99.84% - Val Acc: 83.57% - Time: 16.52s\n",
            "Best model saved with Val Acc: 83.57%\n",
            "Epoch [23/50] - Loss: 0.0048 - Train Acc: 99.94% - Val Acc: 83.63% - Time: 16.72s\n",
            "Best model saved with Val Acc: 83.63%\n",
            "Epoch [24/50] - Loss: 0.0030 - Train Acc: 99.98% - Val Acc: 83.75% - Time: 16.13s\n",
            "Best model saved with Val Acc: 83.75%\n",
            "Epoch [25/50] - Loss: 0.0025 - Train Acc: 99.98% - Val Acc: 83.60% - Time: 16.32s\n",
            "Epoch [26/50] - Loss: 0.0023 - Train Acc: 99.98% - Val Acc: 83.62% - Time: 16.22s\n",
            "Epoch [27/50] - Loss: 0.0020 - Train Acc: 99.98% - Val Acc: 83.86% - Time: 16.57s\n",
            "Best model saved with Val Acc: 83.86%\n",
            "Epoch [28/50] - Loss: 0.0018 - Train Acc: 99.98% - Val Acc: 83.58% - Time: 16.28s\n",
            "Epoch [29/50] - Loss: 0.0014 - Train Acc: 99.99% - Val Acc: 83.60% - Time: 16.10s\n",
            "Epoch [30/50] - Loss: 0.0020 - Train Acc: 99.97% - Val Acc: 83.50% - Time: 16.03s\n",
            "Epoch [31/50] - Loss: 0.0016 - Train Acc: 99.99% - Val Acc: 83.60% - Time: 15.93s\n",
            "Epoch [32/50] - Loss: 0.0017 - Train Acc: 99.98% - Val Acc: 83.63% - Time: 16.45s\n",
            "Epoch [33/50] - Loss: 0.0021 - Train Acc: 99.96% - Val Acc: 83.35% - Time: 15.96s\n",
            "Epoch [34/50] - Loss: 0.0013 - Train Acc: 99.99% - Val Acc: 83.57% - Time: 16.13s\n",
            "Epoch [35/50] - Loss: 0.0034 - Train Acc: 99.91% - Val Acc: 82.76% - Time: 16.42s\n",
            "Epoch [36/50] - Loss: 0.0031 - Train Acc: 99.94% - Val Acc: 83.06% - Time: 16.33s\n",
            "Epoch [37/50] - Loss: 0.0014 - Train Acc: 99.99% - Val Acc: 83.52% - Time: 16.18s\n",
            "Epoch [38/50] - Loss: 0.0013 - Train Acc: 99.99% - Val Acc: 83.52% - Time: 16.08s\n",
            "Epoch [39/50] - Loss: 0.0011 - Train Acc: 99.99% - Val Acc: 83.53% - Time: 16.29s\n",
            "Epoch [40/50] - Loss: 0.0026 - Train Acc: 99.95% - Val Acc: 83.02% - Time: 16.08s\n",
            "Epoch [41/50] - Loss: 0.0018 - Train Acc: 99.98% - Val Acc: 83.34% - Time: 16.55s\n",
            "Epoch [42/50] - Loss: 0.0014 - Train Acc: 99.98% - Val Acc: 83.41% - Time: 16.28s\n",
            "Epoch [43/50] - Loss: 0.0010 - Train Acc: 99.99% - Val Acc: 83.63% - Time: 16.30s\n",
            "Epoch [44/50] - Loss: 0.0010 - Train Acc: 99.99% - Val Acc: 83.49% - Time: 16.18s\n",
            "Epoch [45/50] - Loss: 0.0010 - Train Acc: 99.99% - Val Acc: 83.53% - Time: 16.58s\n",
            "Epoch [46/50] - Loss: 0.0009 - Train Acc: 99.99% - Val Acc: 83.46% - Time: 16.37s\n",
            "Epoch [47/50] - Loss: 0.0008 - Train Acc: 100.00% - Val Acc: 83.40% - Time: 16.46s\n",
            "Epoch [48/50] - Loss: 0.0008 - Train Acc: 100.00% - Val Acc: 83.73% - Time: 16.48s\n",
            "Epoch [49/50] - Loss: 0.0008 - Train Acc: 100.00% - Val Acc: 83.63% - Time: 16.57s\n",
            "Epoch [50/50] - Loss: 0.0007 - Train Acc: 100.00% - Val Acc: 83.65% - Time: 16.85s\n",
            "\n",
            "Training Completed!\n",
            "Best Validation Accuracy: 83.86%\n",
            "PyTorch Version: 2.5.1+cu121\n",
            "\n",
            "Final Test Accuracy: 83.86%\n",
            "Confusion Matrix:\n",
            "[[876   8  24   9   8   3   2  14  37  19]\n",
            " [  6 927   4   2   2   3   3   1   6  46]\n",
            " [ 57   0 729  39  53  46  39  26   6   5]\n",
            " [ 19   1  42 692  40 125  38  25   9   9]\n",
            " [ 13   3  33  48 829  21  22  26   4   1]\n",
            " [  7   0  26 127  34 757  13  27   3   6]\n",
            " [  6   4  22  44  17  16 875   4   7   5]\n",
            " [  7   3  17  26  32  42   1 862   1   9]\n",
            " [ 26  12   5   6   2   1   4   7 916  21]\n",
            " [ 20  26   4   5   1   2   1   5  13 923]]\n"
          ]
        }
      ]
    }
  ]
}